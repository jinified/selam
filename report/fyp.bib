@article{Paden2016,
abstract = {Self-driving vehicles are a maturing technology with the potential to reshape mobility by enhancing the safety, accessibility, efficiency, and convenience of automotive transportation. Safety-critical tasks that must be executed by a self-driving vehicle include planning of motions through a dynamic environment shared with other vehicles and pedestrians, and their robust executions via feedback control. The objective of this paper is to survey the current state of the art on planning and control algorithms with particular regard to the urban setting. A selection of proposed techniques is reviewed along with a discussion of their effectiveness. The surveyed approaches differ in the vehicle mobility model used, in assumptions on the structure of the environment, and in computational requirements. The side-by-side comparison presented in this survey helps to gain insight into the strengths and limitations of the reviewed approaches and assists with system level design choices.},
archivePrefix = {arXiv},
arxivId = {1604.07446},
author = {Paden, Brian and Cap, Michal and Yong, Sze Zheng and Yershov, Dmitry and Frazzoli, Emilio},
doi = {10.1109/TIV.2016.2578706},
eprint = {1604.07446},
file = {:home/batumon/Desktop/fyp/resources/papers/plan{\_}A Survey of Motion Planning and Control
Techniques for Self-driving Urban Vehicles.pdf:pdf},
issn = {2379-8904},
pages = {1--27},
title = {{A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles}},
url = {http://arxiv.org/abs/1604.07446},
year = {2016}
}
@inproceedings{quigley2009ros,
  title={ROS: an open-source Robot Operating System},
  author={Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y},
  booktitle={ICRA workshop on open source software},
  volume={3},
  number={3.2},
  pages={5},
  year={2009},
  organization={Kobe, Japan}
}
@inproceedings{nnolim2008homomorphic,
  title={Homomorphic Filtering of colour images using a Spatial Filter Kernel in the HSI colour space},
  author={Nnolim, Uche and Lee, Peter},
  booktitle={Instrumentation and Measurement Technology Conference Proceedings, 2008. IMTC 2008. IEEE},
  pages={1738--1743},
  year={2008},
  organization={IEEE}
}
@article{collins2005online,
  title={Online selection of discriminative tracking features},
  author={Collins, Robert T and Liu, Yanxi and Leordeanu, Marius},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={10},
  pages={1631--1643},
  year={2005},
  publisher={IEEE}
}
@article{Chau2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.2687v1},
author = {Chau, Duc Phu and Thonnat, Monique and Br{\'{e}}mond, Fran{\c{c}}ois},
doi = {10.1007/978-3-642-39402-7_25},
eprint = {arXiv:1305.2687v1},
file = {:home/batumon/Desktop/fyp/resources/papers/aut{\_}Automatic Parameter Adaptation for
Multi-Object Tracking.pdf:pdf},
isbn = {9783642394010},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Object tracking,controller,machine learning,parameter adaptation},
pages = {244--253},
title = {{Automatic parameter adaptation for multi-object tracking}},
volume = {7963 LNCS},
year = {2013}
}
@article{Jaegle2016,
abstract = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.},
archivePrefix = {arXiv},
arxivId = {1602.04886},
author = {Jaegle, Andrew and Phillips, Stephen and Daniilidis, Kostas},
doi = {10.1109/ICRA.2016.7487206},
eprint = {1602.04886},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaegle, Phillips, Daniilidis - 2016 - Fast, Robust, Continuous Monocular Egomotion Computation.pdf:pdf},
isbn = {9781467380256},
issn = {10504729},
title = {{Fast, Robust, Continuous Monocular Egomotion Computation}},
url = {http://arxiv.org/abs/1602.04886},
year = {2016}
}
@article{Firouzi2010,
abstract = {This paper presents a real-time vision-based object tracking system consisting of a camera on a 2-DOF manipulator which, for example, can be a PT camera. The main novelties of the proposed tracking system include the ability to i) reduce the image processing load by relying on the object position as the only feature of the images acquired from a camera, and ii) estimate the distance and motion of the object without the need for an active rangefinder. The object tracking system is capable of controlling a manipulator using a feedback system based on the object position. The control rule of the feedback system is to minimize the distance between the object position in the camera image and the center point of the image. The proposed method can be readily adopted in dynamic environments, and achieve better tracking accuracies and efficiencies than the traditional methods. The formulation of the distance and motion estimation is presented in the paper. The efficiency and robustness of the proposed method in dealing with noise is verified by simulation using a PT camera.},
author = {Firouzi, H. and Najjaran, H.},
doi = {10.1109/AIM.2010.5695936},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}Real-time	monocular	vision-based	object
tracking	with	object	distance	and	motion
estimation.pdf:pdf},
isbn = {9781424480319},
journal = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM},
number = {July},
pages = {987--992},
title = {{Real-time monocular vision-based object tracking with object distance and motion estimation}},
year = {2010}
}
@article{Cai2015,
abstract = {In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) method with three stages for multiphase segmentation of color images corrupted by different degradations: noise, information loss, and blur. At the first stage, a convex variant of the Mumford-Shah model is applied to each channel to obtain a smooth image. We show that the model has unique solution under the different degradations. In order to properly handle the color information, the second stage is dimension lifting where we consider a new vector-valued image composed of the restored image and its transform in the secondary color space with additional information. This ensures that even if the first color space has highly correlated channels, we can still have enough information to give good segmentation results. In the last stage, we apply multichannel thresholding to the combined vector-valued image to find the segmentation. The number of phases is only required in the last stage, so users can choose or change it all without the need of solving the previous stages again. Experiments demonstrate that our SLaT method gives excellent results in terms of segmentation quality and CPU time in comparison with other state-of-the-art segmentation methods.},
archivePrefix = {arXiv},
arxivId = {1506.00060},
author = {Cai, Xiaohao and Chan, Raymond and Nikolova, Mila and Zeng, Tieyong},
eprint = {1506.00060},
file = {:home/batumon/Desktop/fyp/resources/papers/seg{\_}A Three-stage Approach for Segmenting
Degraded Color Images$\backslash$: Smoothing, Lifting
and Thresholding (SLaT).pdf:pdf},
number = {4053007},
pages = {1--19},
title = {{A Three-stage Approach for Segmenting Degraded Color Images: Smoothing, Lifting and Thresholding (SLaT)}},
url = {http://arxiv.org/abs/1506.00060},
year = {2015}
}
@article{Saxena2008,
abstract = {We consider the task of 3-d depth estimation from a single still image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured indoor and outdoor environments which include forests, sidewalks, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the value of the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a hierarchical, multiscale Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models the depths and the relation between depths at different points in the image. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps. We further propose a model that incorporates both monocular cues and stereo (triangulation) cues, to obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone.},
author = {Saxena, Ashutosh and Chung, Sung H. and Ng, Andrew Y.},
doi = {10.1007/s11263-007-0071-y},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}3-D Depth Reconstruction from a Single Still Image.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {3D reconstruction,Dense reconstruction,Depth estimation,Hand-held camera,Learning depth,Markov random field,Monocular depth,Monocular vision,Stereo vision,Visual modeling},
number = {1},
pages = {53--69},
pmid = {15805533},
title = {{3-D depth reconstruction from a single still image}},
volume = {76},
year = {2008}
}
@article{Garcia2013,
author = {Garc{\'{i}}a, Fernando and Escalera, Arturo De and Armingol, Jos{\'{e}} Mar{\'{i}}a},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Enhanced Obstacle Detection based on Data Fusion for ADAS
Applications.pdf:pdf},
isbn = {9781479929146},
journal = {16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)},
number = {Itsc},
pages = {1370--1375},
title = {{Enhanced Obstacle Detection based on Data Fusion for ADAS Applications}},
year = {2013}
}
@article{Benoit2014,
abstract = {Team S.O.N.I.A. is an undergraduate team of engineering students from {\'{E}}cole de Technologie Sup{\'{e}}rieure (ETS) in Montreal, Quebec. S.O.N.I.A. stands for “Syst{\`{e}}me d'Op{\'{e}}ration Nautique Intelligent et Autonome”, which translates to Intelligent and Autonomous Nautical Operation System. The purpose of the team is to build an Autonomous Underwater Vehicle (AUV), which is built to perform an obstacle course at the 17th annual Robosub competition held in San Diego, California. The course simulates real tasks that would be assigned to an AUV in real life, such as pipe inspection, precise navigation, image and pattern recognition and acoustic localisation. In order to accomplish those tasks, the submarine has several navigation sensors, such as an Inertial Navigation System (INS) and a Doppler Velocity Logger (DVL). It is also equipped with two cameras, mechanical imaging sonar and four hydrophones. In order to move in the water, the submarine uses six brushless thrusters, allowing navigation in five degrees of freedom. Major improvements have been made to the control system, resulting in the most precise navigation system Team S.O.N.I.A. has ever had.},
author = {Benoit, Mathieu and Goulet, Marc-Andr{\'{e}} Bolduc and Bouchard-d'Haese, Florence and Bouzidi, Racha and Carrier, Julien and Couturier, {\'{E}}lodie and Desjardins, Vincent and Dozois, Antoine and Fortier, Mathieu and Langlois, Fr{\'{e}}d{\'{e}}ric and Ritchie, Karl and Pr{\'{e}}vost, J{\'{e}}r{\'{e}}mie St-Jules},
file = {:home/batumon/Desktop/fyp/resources/related/jpaper{\_}sonia{\_}2014.pdf:pdf},
title = {{S.O.N.I.A - Autonomous Underwater Vehicle}},
url = {https://www.google.pt/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}cad=rja{\&}uact=8{\&}ved=0ahUKEwizvtGG8uXLAhWEchQKHej5CJwQFggeMAA{\&}url=http://sonia.etsmtl.ca/wp-content/uploads/jpaper{\_}sonia{\_}2014.pdf{\&}usg=AFQjCNFoHtuacrHpN0{\_}rNPN3AdnhhhJMYA{\&}sig2=tp2{\_}PRkI},
year = {2014}
}
@article{Zhu2014,
abstract = {Recent progresses in salient object detection have exploited the boundary prior, or background information, to assist other saliency cues such as contrast, achieving state- of-the-art results. However, their usage of boundary prior is very simple, fragile, and the integration with other cues is mostly heuristic. In this work, we present new methods to address these issues. First, we propose a robust background measure, called boundary connectivity. It characterizes the spatial layout of image regions with respect to image boundaries and is much more robust. It has an intuitive geometrical interpretation and presents unique benefit- s that are absent in previous saliency measures. Second, we propose a principled optimization framework to integrate multiple low level cues, including our background measure, to obtain clean and uniform saliency maps. Our formulation is intuitive, efficient and achieves state-of-the-art results on several benchmark datasets.},
author = {Zhu, Wangjiang and Liang, Shuang and Wei, Yichen and Sun, Jian},
doi = {10.1109/CVPR.2014.360},
file = {:home/batumon/Desktop/fyp/resources/papers/Saliency Optimization from Robust Background Detection.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2814--2821},
title = {{Saliency optimization from robust background detection}},
year = {2014}
}
@article{Zhu2015,
abstract = {Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.},
archivePrefix = {arXiv},
arxivId = {1507.08085},
author = {Zhu, Gao and Porikli, Fatih and Li, Hongdong},
eprint = {1507.08085},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Tracking Randomly Moving Objects on Edge Box Proposals.pdf:pdf},
journal = {arXiv},
title = {{Tracking Randomly Moving Objects on Edge Box Proposals}},
url = {http://arxiv.org/abs/1507.08085},
year = {2015}
}
@article{Kaiming2011,
abstract = {In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.},
author = {Kaiming, He and Jian, Sun and Xiaoou, Tang},
doi = {10.1109/TPAMI.2010.168},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Single Image Haze Removal Using Dark Channel Prior.pdf:pdf},
isbn = {1939-3539 (Electronic)0098-5589 (Linking)},
journal = {IEEE Trans Pattern Anal Mach Intell},
number = {12},
pages = {2341--2353},
pmid = {20820075},
title = {{Single Image Haze Removal Using Dark Channel Prior}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20820075},
volume = {33},
year = {2011}
}
@article{Hosang2015,
abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL and ImageNet, and impact on DPM and R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detector performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
archivePrefix = {arXiv},
arxivId = {1502.05082},
author = {Hosang, Jan and Benenson, Rodrigo and Doll{\'{a}}r, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
eprint = {1502.05082},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}What makes for effective detection proposals?.pdf:pdf},
issn = {0162-8828},
journal = {Arxiv},
pages = {2014},
title = {{What makes for effective detection proposals?}},
url = {http://arxiv.org/abs/1502.05082},
year = {2015}
}
@article{Zhang2013,
author = {Zhang, B},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Contrast Enhancement and Optimization for Underwater Images.pdf:pdf},
keywords = {- underwater image,absorption,and,beam,hardening,image optimization,intensity from the imaged,light intensity received by,object surface of underwater,scattering,the,the latter expresses the},
number = {Iccsee},
pages = {921--924},
title = {{Contrast Enhancement and Optimization for Underwater Images}},
year = {2013}
}
@article{Vigo2010,
abstract = {State of the art methods for image matching, content-based retrieval and recognition use local features. Most of these still exploit only the luminance information for detection. The color saliency boosting algorithm has provided an efficient method to exploit the saliency of color edges based on information theory. However, during the design of this algorithm, some issues were not addressed in depth: (1) The method has ignored the underly- ing distribution of derivatives in natural images. (2) The depen- dence of information content in color-boosted edges on its spatial derivatives has not been quantitatively established. (3) To eval- uate luminance and color contributions to saliency of edges, a parameter gradually balancing both contributions is required. We introduce a novel algorithm, based on the principles of independent component analysis, which models the first or- der derivatives of color natural images by a generalized Gaus- sian distribution. Furthermore, using this probability model we show that for images with a Laplacian distribution, which is a particular case of generalized Gaussian distribution, the mag- nitudes of color-boosted edges reflect their corresponding infor- mation content. In order to evaluate the impact of color edge saliency in real world applications, we introduce an extension of the Laplacian-of-Gaussian detector to color, and the perfor- mance for image matching is evaluated. Our experiments show that our approach provides more discriminative regions in com- parison with the original detector.},
author = {Vigo, Dr and Weijer, Joost Van De and Gevers, T},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Color Edge Saliency Boosting using Natural Image Statistics.pdf:pdf},
isbn = {9781617388897},
journal = {4th European Conference on Colour in Graphics, Imaging, and Vision},
number = {1},
pages = {228--234},
title = {{Color edge saliency boosting using natural image statistics}},
url = {http://dare.uva.nl/record/368523},
year = {2010}
}
@article{Huang,
author = {Huang, Jia-bin and Caruana, Rich and Farnsworth, Andrew and Kelling, Steve and Ahuja, Narendra},
doi = {10.1109/CVPR.2016.230},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Detecting Migrating Birds at Night.pdf:pdf},
pages = {2091--2099},
title = {{Detecting Migrating Birds at Night}}
}
@article{Bazzani2012,
author = {Bazzani, Loris},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}BeyongMulti-Target-Tracking.pdf:pdf},
title = {{Beyond Multi-target Tracking}},
year = {2012}
}
@article{Fei-Fei2006,
abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
doi = {10.1109/TPAMI.2006.79},
file = {:home/batumon/Desktop/fyp/resources/papers/ml{\_}One-Shot Learning of Object Categories.pdf:pdf},
isbn = {0162-8828 VO - 28},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Few images,Learning,Object categories,Priors,Recognition,Unsupervised,Variational inference},
number = {4},
pages = {594--611},
pmid = {16566508},
title = {{One-shot learning of object categories}},
volume = {28},
year = {2006}
}
@article{Ancuti2011,
author = {Ancuti, Cosmin and Bekaert, Philippe},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Enhancing Underwater Images and Videos by Fusion.pdf:pdf},
isbn = {9781450309219},
journal = {Computer},
pages = {4503--4503},
title = {{Enhancing Underwater Images by Fusion}},
year = {2011}
}
@article{Kalal2012,
abstract = {This paper investigates long-term tracking of unknown objects in a video stream. The object is defined by its location and extent in a single frame. In every frame that follows, the task is to determine the object's location and extent or indicate that the object is not present. We propose a novel tracking framework (TLD) that explicitly decomposes the long-term tracking task into tracking, learning and detection. The tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary. The learning estimates detector's errors and updates it to avoid these errors in the future. We study how to identify detector's errors and learn from them. We develop a novel learning method (P-N learning) which estimates the errors by a pair of "experts'': (i) P-expert estimates missed detections, and (ii) N-expert estimates false alarms. The learning process is modeled as a discrete dynamical system and the conditions under which the learning guarantees improvement are found. We describe our real-time implementation of the TLD framework and the P-N learning. We carry out an extensive quantitative evaluation which shows a significant improvement over state-of-the-art approaches.},
author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/TPAMI.2011.239},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Tracking-Learning-Detection.pdf:pdf},
isbn = {2011030153},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Long-term tracking,bootstrapping,learning from video,real time,semi-supervised learning},
number = {7},
pages = {1409--1422},
pmid = {22156098},
title = {{Tracking-learning-detection}},
volume = {34},
year = {2012}
}
@article{Tao2016,
abstract = {In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.},
archivePrefix = {arXiv},
arxivId = {1605.05863},
author = {Tao, Ran and Gavves, Efstratios and Smeulders, Arnold W. M.},
doi = {10.1109/CVPR.2016.158},
eprint = {1605.05863},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Siamese Instance Search for Tracking.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1420--1429},
title = {{Siamese Instance Search for Tracking}},
url = {http://arxiv.org/abs/1605.05863},
year = {2016}
}
@article{Bani2013,
author = {Bani, Nikola},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Using the Random Sprays Retinex algorithm for
global illumination estimation.pdf:pdf},
keywords = {7,8,9,and com-,bination of existing methods,color constancy,hlvi,information,mentioned al-,probabilistic algorithms,random,retinex,sampling,sprays retinex,the result of all,white balance},
pages = {3--8},
title = {{Using the Random Sprays Retinex algorithm for global illumination estimation}},
year = {2013}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
eprint = {arXiv:1407.5736v1},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}Parallel Tracking and Mapping for Small AR Workspaces.pdf:pdf},
isbn = {9781424417506},
issn = {00472778},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
pmid = {21736739},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}
@article{Avidan2005,
author = {Avidan, Shai},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Ensemble Tracking.pdf:pdf},
title = {{Ensemble Tracking .pdf}},
year = {2005}
}
@article{Shihavuddin2012,
author = {Shihavuddin, ASM and Gracias, Nuno and Garc{\'{i}}a, R},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Online	sunflicker	removal	using	dynamic
texture	prediction.pdf:pdf},
isbn = {9789898565037},
journal = {Visapp (1)},
number = {January},
title = {{Online Sunflicker Removal using Dynamic Texture Prediction.}},
url = {http://users.isr.ist.utl.pt/{~}ngracias/publications/Shihav12{\_}visapp2012.pdf},
year = {2012}
}
@article{Grabner2006,
abstract = {Very recently tracking was approached using classification techniques such as support vector machines. The object to be tracked is discriminated by a classifier from the background. In a similar spirit we propose a novel on-line AdaBoost feature selection algorithm for tracking. The distinct advantage of our method is its capability of on-line training. This allows to adapt the classifier while tracking the object. Therefore appearance changes of the object (e.g. out of plane rotations, illumination changes) are handled quite naturally. Moreover, depending on the background the algorithm selects the most discriminating features for tracking resulting in stable tracking results. By using fast computable features (e.g. Haar-like wavelets, orientation histograms, local binary patterns) the algorithm runs in real-time. We demonstrate the performance of the algorithm on several (publically available) video sequences.},
author = {Grabner, Helmut and Grabner, Michael and Bischof, Horst},
doi = {10.5244/C.20.6},
file = {:home/batumon/Desktop/fyp/resources/papers/real{\_}time{\_}tracking{\_}boosting.pdf:pdf},
isbn = {1-901725-32-4},
issn = {0162-8828},
journal = {Proceedings of the British Machine Vision Conference},
pages = {1--10},
title = {{Real-Time Tracking via On-line Boosting}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.8743{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {2006}
}
@article{Drews-Jr2013,
abstract = {This paper proposes a methodology to estimate the trans- mission in underwater environments which consists on an adaptation of the Dark Channel Prior (DCP), a statisti- cal prior based on properties of images obtained in out- door natural scenes. Our methodology, called Underwa- ter DCP (UDCP), basically considers that the blue and green color channels are the underwater visual information source, which enables a significant improvement over exist- ing methods based in DCP. This is shown through a com- parative study with state of the art techniques, we present a detailed analysis of our technique which shows its appli- cability and limitations in images acquired from real and simulated scenes. 1.},
author = {Drews-Jr, P. and {Do Nascimento}, E. and Moraes, F. and Botelho, S. and Campos, M.},
doi = {10.1109/ICCVW.2013.113},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Transmission Estimation in Underwater Single Images.pdf:pdf},
isbn = {9781479930227},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {1},
pages = {825--830},
title = {{Transmission estimation in underwater single images}},
year = {2013}
}
@article{Bertinetto2015,
abstract = {Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations such as motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 90 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.},
archivePrefix = {arXiv},
arxivId = {1512.01355},
author = {Bertinetto, Luca and Valmadre, Jack and Golodetz, Stuart and Miksik, Ondrej and Torr, Philip},
doi = {10.1109/CVPR.2016.156},
eprint = {1512.01355},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Staple$\backslash$: Complementary Learners for Real-Time Tracking.pdf:pdf},
journal = {arXiv},
pages = {1401--1409},
title = {{Staple: Complementary Learners for Real-Time Tracking}},
url = {http://arxiv.org/abs/1512.01355},
year = {2015}
}
@article{Hariharan2016,
abstract = {Low-shot visual learning - the ability to recognize novel object categories from very few examples - is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a novel protocol to evaluate low-shot learning on complex images where the learner is permitted to first build a feature representation. Then, we propose and evaluate representation regularization techniques that improve the effectiveness of convolutional networks at the task of low-shot learning, leading to a 2x reduction in the amount of training data required at equal accuracy rates on the challenging ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02819},
author = {Hariharan, Bharath and Girshick, Ross},
eprint = {1606.02819},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}det{\_}Low-shot visual object recognition.pdf:pdf},
pages = {1--10},
title = {{Low-shot visual object recognition}},
url = {http://arxiv.org/abs/1606.02819},
year = {2016}
}
@article{Lea,
author = {Lea, Colin and Corso, Jason J},
file = {:home/batumon/Desktop/fyp/resources/papers/ml{\_}Efficient Hierarchical Markov Random Fields
for Object Detection on a Mobile Robot.pdf:pdf},
title = {{Efficient Hierarchical Markov Random Fields for Object Detection on a Mobile Robot}}
}
@article{Rubino2015,
abstract = {We present a novel method to infer, in closed-form, a general 3D spatial occupancy and orientation of a collection of rigid objects given 2D image detections from a sequence of images. In particular, starting from 2D ellipses fitted to bounding boxes, this novel multi-view problem can be reformulated as the estimation of a quadric (ellipsoid) in 3D. We show that an efficient solution exists in the dual-space using a minimum of three views while a solution with two views is possible through the use of regularization. However, this algebraic solution can be negatively affected in the presence of gross inaccuracies in the bounding boxes estimation. To this end, we also propose a robust ellipse fitting algorithm able to improve performance in the presence of errors in the detected objects. Results on synthetic tests and on different real datasets, involving real challenging scenarios, demonstrate the applicability and potential of our method.},
archivePrefix = {arXiv},
arxivId = {1502.04754},
author = {Rubino, Cosimo and Crocco, Marco and Perina, Alessandro and Murino, Vittorio and {Del Bue}, Alessio},
eprint = {1502.04754},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubino et al. - 2015 - 3D Pose from Detections.pdf:pdf},
title = {{3D Pose from Detections}},
url = {http://arxiv.org/abs/1502.04754},
year = {2015}
}
@article{Wedel2006,
abstract = {This paper deals with the detection of arbitrary static objects in traffic scenes from monocular video using structure from motion. A camera in a moving vehicle observes the road course ahead. The camera translation in depth is known. Many structure from motion algorithms were proposed for detecting moving or nearby objects. However, detecting stationary distant obstacles in the focus of expansion remains quite challenging due to very small subpixel motion between frames. In this work the scene depth is estimated from the scaling of supervised image regions. We generate obstacle hypotheses from these depth estimates in image space. A second step then performs testing of these by comparing with the counter hypothesis of a free driveway. The approach can detect obstacles already at distances of 50m and more with a standard focal length. This early detection allows driver warning and safety precaution in good time.},
author = {Wedel, Andreas and Franke, Uwe and Klappstein, Jens and Brox, Thomas and Cremers, Daniel},
doi = {10.1007/11861898_48},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}Realtime Depth Estimation and Obstacle
Detection from Monocular Video.pdf:pdf},
isbn = {978-3-540-44414-5},
issn = {03029743},
journal = {Pattern Recognition},
pages = {475--484},
title = {{Realtime depth estimation and obstacle detection from monocular video}},
url = {http://link.springer.com/chapter/10.1007/11861898{\_}48},
year = {2006}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/pdf/1512.03385v1.pdf},
volume = {7},
year = {2015}
}
@article{Galdran2015,
abstract = {Underwater images typically exhibit color distortion and low contrast as a result of the exponential decay that light suffers as it travels. Moreover, colors associated to different wavelengths have different attenuation rates, being the red wavelength the one that attenuates the fastest. To restore underwater images, we propose a Red Channel method, where colors associated to short wavelengths are recovered, as expected for underwater images, leading to a recovery of the lost contrast. The Red Channel method can be interpreted as a variant of the Dark Channel method used for images degraded by the atmosphere when exposed to haze. Experimental results show that our technique handles gracefully artificially illuminated areas, and achieves a natural color correction and superior or equivalent visibility improvement when compared to other state-of-the-art methods.},
author = {Galdran, Adrian and Pardo, David and Pic{\'{o}}n, Artzai and Alvarez-Gila, Aitor},
doi = {10.1016/j.jvcir.2014.11.006},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Automatic Red-Channel Underwater Image Restoration.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Artificial lighting,Attenuation,Color correction,Contrast enhancement,Dark Channel,Image dehazing,Underwater image degradation,Underwater image restoration,Visibility recovery},
pages = {132--145},
title = {{Automatic Red-Channel underwater image restoration}},
volume = {26},
year = {2015}
}
@article{Chiang2011,
abstract = {Underwater environments often cause color scatter and color cast during photography. Color scatter is caused by haze effects occurring when light reflected from objects is absorbed or scattered multiple times by particles in the water. This in turn lowers the visibility and contrast of the image. Color cast is caused by the varying attenuation of light in different wavelengths, rendering underwater environments bluish. To address distortion from color scatter and color cast, this study proposes an algorithm to restore underwater images that combines a dehazing algorithm with wavelength compensation (WCID). Once the distance between the objects and the camera was estimated using dark channel prior, the haze effects from color scatter were removed by the dehazing algorithm. Next, estimation of the photography scene depth from the residual energy ratios of each wavelength in the background light of the image was performed. According to the amount of attenuation of each wavelength, reverse compensation was conducted to restore the distortion from color cast. An underwater video downloaded from the Youtube website was processed using WCID, Histogram equalization, and a traditional dehazing algorithm. Comparison of the results revealed that WCID simultaneously resolved the issues of color scatter and color cast as well as enhanced image contrast and calibrated color cast, producing high quality underwater images and videos.},
author = {Chiang, John Y. and Chen, Ying Ching and Chen, Yung Fu},
doi = {10.1007/978-3-642-23687-7_34},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Underwater Image Enhancement$\backslash$: Using Wavelength
Compensation and Image Dehazing (WCID).pdf:pdf},
isbn = {9783642236860},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image dehazing,Underwater image,Wavelength compensation},
pages = {372--383},
title = {{Underwater image enhancement: Using wavelength compensation and image dehazing (WCID)}},
volume = {6915 LNCS},
year = {2011}
}
@article{Pepperell2014,
abstract = {This paper presents Sequence Matching Across Route Traversals (SMART); a generally applicable sequence-based place recognition algorithm. SMART provides invariance to changes in illumination and vehicle speed while also providing moderate pose invariance and robustness to environmental aliasing. We evaluate SMART on vehicles travelling at highly variable speeds in two challenging environments; firstly, on an all-terrain vehicle in an off-road, forest track and secondly, using a passenger car traversing an urban environment across day and night. We provide comparative results to the current state-of-the-art SeqSLAM algorithm and investigate the effects of altering SMART's image matching parameters. Additionally, we conduct an extensive study of the relationship between image sequence length and SMART's matching performance. Our results show viable place recognition performance in both environments with short 10-metre sequences, and up to 96{\%} recall at 100{\%} precision across extreme day-night cycles when longer image sequences are used.},
author = {Pepperell, Edward and Corke, Peter I. and Milford, Michael J.},
doi = {10.1109/ICRA.2014.6907067},
file = {:home/batumon/Desktop/fyp/resources/papers/detect{\_}All-Environment Visual Place Recognition with SMART.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1612--1618},
title = {{All-environment visual place recognition with SMART}},
year = {2014}
}
@article{Sharma2012,
abstract = {Most common approaches for object detection collect thousands of training examples and train a detector in an offline setting, using supervised learning methods, with the objective of obtaining a generalized detector that would give good performance on various test datasets. However, when an offline trained detector is applied on challenging test datasets, it may fail in some cases by not being able to detect some objects or by producing false alarms. We propose an unsupervised multiple instance learning (MIL) based incremental solution to deal with this issue. We introduce an MIL loss function for Real Adaboost and present a tracking based effective unsupervised online sample collection mechanism to collect the online samples for incremental learning. Experiments demonstrate the effectiveness of our approach by improving the performance of a state of the art offline trained detector on the challenging datasets for pedestrian category.},
author = {Sharma, Pramod and Huang, Chang and Nevatia, Ram},
doi = {10.1109/CVPR.2012.6248067},
file = {:home/batumon/Desktop/fyp/resources/papers/unsup{\_}Unsupervised Incremental Learning for Improved Object Detection in a Video.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {Mil},
pages = {3298--3305},
title = {{Unsupervised incremental learning for improved object detection in a video}},
year = {2012}
}
@article{Said2012,
abstract = {This paper briefly discusses the depth estimation method for a mobile platform using monocular vision. The biggest challenge for autonomous mobile platform in an unknown environment is the accuracy in the estimation of the distance and the position of obstacles around them. In order for them to safely navigate from one position to another, reliable range sensors are needed to detect any obstacle that blocked their path. Vision sensor can be used for the purpose, as it can provide a better and cost-effective solution. The method discussed in this paper requires a simple calibration. The data obtained from calibration process will be used to generate the equation for depth estimation procedure. The results presented in this paper testify the reliability of the methodology used for depth estimation.},
author = {Said, Z. and Sundaraj, K. and Wahab, M.N.a.},
doi = {10.1016/j.proeng.2012.07.267},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}Depth Estimation for a Mobile Platform Using Monocular Vision.pdf:pdf},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {depth estimation,monocular vision,pixel location,vision sensor},
number = {Iris},
pages = {945--950},
title = {{Depth Estimation for a Mobile Platform Using Monocular Vision}},
url = {http://dx.doi.org/10.1016/j.proeng.2012.07.267},
volume = {41},
year = {2012}
}
@article{Ren,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v2},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v2},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Faster R-CNN$\backslash$: Towards Real-Time Object Detection
with Region Proposal Networks.pdf:pdf},
issn = {01689002},
pages = {1--9},
title = {{5638-Faster-R-Cnn-Towards-Real-Time-Object-Detection-With-Region-Proposal-Networks}}
}
@article{Yang2010,
abstract = {In this paper, we propose a simple but effective specular highlight removal method using a single input image. Our method is based on a key observation - the maximum fraction of the diffuse color component (so called maximum diffuse chromaticity in the literature) in local patches in color images changes smoothly. Using this property, we can estimate the maximum diffuse chromaticity values of the specular pixels by directly applying low-pass filter to the maximum fraction of the color components of the original image, such that the maximum diffuse chromaticity values can be propagated from the diffuse pixels to the specular pixels. The diffuse color at each pixel can then be computed as a nonlinear function of the estimated maximum diffuse chromaticity. Our method can be directly extended for multi-color surfaces if edge-preserving filters (e.g., bilateral filter) are used such that the smoothing can be guided by the maximum diffuse chromaticity. But maximum diffuse chromaticity is to be estimated. We thus present an approximation and demonstrate its effectiveness. Recent development in fast bilateral filtering techniques enables our method to run over 200x faster than the state-of-the-art on a standard CPU and differentiates our method from previous work.},
author = {Yang, Qingxiong and Wang, Shengnan and Ahuja, Narendra},
doi = {10.1007/978-3-642-15561-1_7},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Real-Time Specular Highlight Removal Using Bilateral
Filtering.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 4},
pages = {87--100},
title = {{Real-time specular highlight removal using bilateral filtering}},
volume = {6314 LNCS},
year = {2010}
}
@article{Saxena2006,
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
doi = {10.1007/s11263-007-0071-y},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}Learning Depth from Single Monocular Images.pdf:pdf},
isbn = {9780262232531},
issn = {0920-5691},
journal = {Advances in Neural Information Processing Systems},
pages = {1161--1168},
title = {{Learning Depth from Single Monocular Images}},
volume = {18},
year = {2006}
}
@article{Cannon2012,
author = {Cannon, Jarad and Rose, Kevin and Ruml, Wheeler},
file = {:home/batumon/Desktop/fyp/resources/papers/plan{\_}Real-Time Motion Planning with Dynamic Obstacles.pdf:pdf},
journal = {Socs},
keywords = {Full Papers},
pages = {33--40},
title = {{Real-Time Motion Planning with Dynamic Obstacles.}},
url = {http://www.aaai.org/ocs/index.php/SOCS/SOCS12/paper/view/5412/5174},
year = {2012}
}
@article{Itti1998,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Itti, Laurent and Koch, Christof and Niebur, Ernst},
doi = {10.1109/34.730558},
eprint = {arXiv:1011.1669v3},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}A{\_}Model{\_}of{\_}Saliency-based{\_}Attention{\_}for{\_}Rapid{\_}Scene{\_}Analysis.pdf:pdf},
isbn = {9788578110796},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Feature extraction,Scene analysis,Target detection,Visual attention,Visual search},
number = {11},
pages = {1254--1259},
pmid = {25246403},
title = {{A model of saliency-based visual attention for rapid scene analysis}},
volume = {20},
year = {1998}
}
@article{Di2015,
author = {Di, Dottorato and In, Ricerca},
file = {:home/batumon/Desktop/fyp/resources/related/Computer Vision Applied To
Underwater Robotics.pdf:pdf},
title = {{Computer Vision Applied To Underwater Robotics}},
year = {2015}
}
@article{Jiang2013,
author = {Jiang, Huaizu and Wang, Jingdong and Yuan, Zejian and Wu, Yang and Zheng, Nanning and Li, Shipeng},
file = {:home/batumon/Desktop/fyp/resources/papers/Salient Object Detection$\backslash$: A Discriminative Regional Feature
Integration Approach.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2083--2090},
title = {{Salient Object Detection: {\{}A{\}} Discriminative Regional Feature Integration Approach}},
year = {2013}
}
@article{Tang2015,
abstract = {Con la amplia disponibilidad de tel{\'{e}}fonos celulares y c{\'{a}}maras que tienen capacidades de GPS, es com{\'{u}}n que las im{\'{a}}genes que se est{\'{a}}n subidos a Internet hoy en d{\'{i}}a tener coordenadas GPS asociado con ellos. Adem{\'{a}}s de la investigaci{\'{o}}n que trata de predecir las coordenadas GPS de las caracter{\'{i}}sticas visuales, esto tambi{\'{e}}n abre la puerta a los problemas que est{\'{a}}n condicionados a la disponibilidad de coordenadas GPS. En este trabajo, abordamos el problema de llevar a cabo la clasificaci{\'{o}}n de im{\'{a}}genes con el contexto de la ubicaci{\'{o}}n, en la que se nos da las coordenadas GPS para las im{\'{a}}genes, tanto en las fases de tren y de prueba. Exploramos diferentes formas de codificaci{\'{o}}n y caracter{\'{i}}sticas de extracci{\'{o}}n de las coordenadas GPS, y mostramos c{\'{o}}mo incorporar de forma natural estas caracter{\'{i}}sticas en una Red Neural convolucional (CNN), el estado de la t{\'{e}}cnica actual para la mayor{\'{i}}a de los problemas de clasificaci{\'{o}}n de im{\'{a}}genes y reconocimiento. Tambi{\'{e}}n mostramos c{\'{o}}mo es posible aprender de manera simult{\'{a}}nea los radios {\'{o}}ptima puesta en com{\'{u}}n para un subconjunto de nuestras funciones en el marco de CNN. Para evaluar nuestro modelo y para ayudar a promover la investigaci{\'{o}}n en esta {\'{a}}rea, se identifican un conjunto de conceptos sensibles a la ubicaci{\'{o}}n y anotar un subconjunto del conjunto de datos de Yahoo Flickr Creative Commons 100M que tiene coordenadas GPS con estos conceptos, que ponemos a disposici{\'{o}}n del p{\'{u}}blico. Al aprovechar contexto ubicaci{\'{o}}n, somos capaces de lograr casi un aumento de 7{\%} en la precisi{\'{o}}n media media.},
archivePrefix = {arXiv},
arxivId = {1505.03873},
author = {Tang, Kevin and Paluri, Manohar and Fei-Fei, Li and Fergus, Rob and Bourdev, Lubomir},
doi = {10.1109/ICCV.2015.121},
eprint = {1505.03873},
file = {:home/batumon/Desktop/fyp/resources/papers/class{\_}Tang{\_}Improving{\_}Image{\_}Classification{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {International Conference on Computer Vision},
keywords = {Kevin Tang,Li Fei-Fei,Lubomir Bourdev,Manohar Paluri,Rob Fergus},
pages = {1008--1016},
title = {{Improving Image Classification with Location Context}},
url = {http://arxiv.org/abs/1505.03873},
year = {2015}
}
@article{Siagian2014,
abstract = {While impressive recent progress has been achieved with autonomous vehicles both indoors and on streets, autonomous localization and navigation in less constrained andmore dynamic environments, such as outdoor pedestrian and bicycle-friendly sites, remains a challenging problem. We describe a new approach that utilizes several visual perception modules — place recognition, landmark recognition, and road lane detection — supplemented by prox- imity cues from a planar Laser Range Finder for obstacle avoidance. At the core of our system is a new hybrid topological/grid-occupancy map which integrates the outputs from all perceptual modules, despite different latencies and timescales. Our approach allows for real-time performance through a combination of fast but shallow processing modules that update the map's state while slower but more discriminating modules are still computing. We validated our system using a ground vehicle that autonomously traversed several times three outdoor routes, each 400m or longer, in a university campus. The routes featured dif- ferent road types, environmental hazards, moving pedestrians, and service vehicles. In total, the robot logged over 10km of successful recorded experiments, driving within a median of 1.37mlaterally of the center of the road, and localizing within 0.97m(median) longitudinally of its true location along the route. 1},
author = {Siagian, Christian and Chang, Chin Kai and Itti, Laurent},
doi = {10.1002/rob.21505},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Autonomous{\_}Mobile{\_}Robot{\_}Localization{\_}And{\_}Navigation{\_}Using{\_}Hierarchical{\_}Vision{\_}Map.pdf:pdf},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {3},
pages = {408--414},
title = {{Autonomous mobile robot localization and navigation using a hierarchical map representation primarily guided by vision}},
volume = {31},
year = {2014}
}
@article{Chang2010,
abstract = {We present a vision-based navigation and localization system using two biologically-inspired scene understanding models which are studied from human visual capabilities: (1) Gist model which captures the holistic characteristics and layout of an image and (2) Saliency model which emulates the visual attention of primates to identify conspicuous regions in the image. Here the localization system utilizes the gist features and salient regions to accurately localize the robot while the navigation system uses the salient regions to perform visual feedback control to direct its heading and go to a user-provided goal location. We tested the system on our robot, Beobot2.0, in an indoor and outdoor environment with a route length of 36.67m (10,890 video frames) and 138.27m (28,971 frames), respectively. On average, the robot is able to drive within 3.68cm and 8.78cm (respectively) of the center of the lane. {\textcopyright}2010 IEEE.},
author = {Chang, Chin Kai and Siagian, Christian and Itti, Laurent},
doi = {10.1109/IROS.2010.5649136},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Mobile Robot Vision Navigation {\&} Localization Using Gist and Saliency.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
pages = {4147--4154},
title = {{Mobile robot vision navigation {\&} localization using gist and saliency}},
year = {2010}
}
@article{Zhuo2009,
abstract = {In this paper we address the challenging problem of recovering the depth of a scene from a single image using defocus cue. To achieve this, we first present a novel approach to estimate the amount of spatially varying defocus blur at edge locations. We re-blur the input image and show that the gradient magnitude ratio between the input and re-blurred images depends only on the amount of defocus blur. Thus, the blur amount can be obtained from the ratio. A layered depth map is then extracted by propagating the blur amount at edge locations to the entire image. Experimental results on synthetic and real images demonstrate the effectiveness of our method in providing a reliable estimate of the depth of a scene.},
author = {Zhuo, Shaojie and Sim, Terence},
doi = {10.1007/978-3-642-03767-2_108},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}On the Recovery of Depth from a Single
Defocused Image.pdf:pdf},
isbn = {3642037666},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Defocus blur,Depth recovery,Gaussian gradient,Image processing,Markov random field},
pages = {889--897},
title = {{On the recovery of depth from a single defocused image}},
volume = {5702 LNCS},
year = {2009}
}
@article{Song,
abstract = {Scale drift is a crucial challenge for monocular au-tonomous driving to emulate the performance of stereo. This paper presents a real-time monocular SFM system that cor-rects for scale drift using a novel cue combination framework for ground plane estimation, yielding accuracy comparable to stereo over long driving sequences. Our ground plane estima-tion uses multiple cues like sparse features, dense inter-frame stereo and (when applicable) object detection. A data-driven mechanism is proposed to learn models from training data that relate observation covariances for each cue to error be-havior of its underlying variables. During testing, this allows per-frame adaptation of observation covariances based on relative confidences inferred from visual data. Our frame-work significantly boosts not only the accuracy of monocular self-localization, but also that of applications like object lo-calization that rely on the ground plane. Experiments on the KITTI dataset demonstrate the accuracy of our ground plane estimation, monocular SFM and object localization relative to ground truth, with detailed comparisons to prior art.},
author = {Song, Shiyu and Chandraker, Manmohan},
file = {:home/batumon/Desktop/fyp/resources/papers/car{\_}Robust Scale Estimation in Real-Time Monocular SFM for Autonomous Driving.pdf:pdf},
title = {{Robust Scale Estimation in Real-Time Monocular SFM for Autonomous Driving}}
}
@article{Bryant2000,
author = {Bryant, Matthew},
file = {:home/batumon/Desktop/fyp/resources/related/3-D Vision for an
Autonomous Underwater
Vehicle.pdf:pdf},
journal = {Camera},
number = {June},
title = {{3-D Vision for an Autonomous Underwater}},
year = {2000}
}
@article{Zhu2016,
abstract = {Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of "high-quality" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector. Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.},
archivePrefix = {arXiv},
arxivId = {1605.01839},
author = {Zhu, Gao and Porikli, Fatih and Li, Hongdong},
doi = {10.1109/CVPR.2016.108},
eprint = {1605.01839},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Beyond Local Search$\backslash$: Tracking Objects Everywhere with Instance-Specific
Proposals.pdf:pdf},
pages = {943--951},
title = {{Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals}},
url = {http://arxiv.org/abs/1605.01839},
year = {2016}
}
@article{Chen2014,
abstract = {Abstract—Over these years, Correlation Filter-based Trackers (CFTs) have aroused increasing interests in the field of visual object tracking, and have achieved extremely compelling results in different competitions and benchmarks. In this paper, our goal is to review the developments of CFTs with extensive experimental results. 11 trackers are surveyed in our work, based on which a general framework is summarized. Furthermore, we investigate different training schemes for correlation filters, and also discuss various effective improvements that have been made recently. Comprehensive experiments have been conducted to evaluate the effectiveness and efficiency of the surveyed CFTs, and comparisons have been made with other competing trackers. The experimental results have shown that state-of- art performence, in terms of robustness, speed and accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF. We find that further improvements for correlation filter- based tracking can be made on estimating scales, applying part- based tracking strategy and cooperating with long-term tracking methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.05520v1},
author = {Chen, Zhe and Hong, Zhibin and Tao, Dacheng},
eprint = {arXiv:1509.05520v1},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}An Experimental Survey on Correlation Filter-based
Tracking.pdf:pdf},
journal = {arXiv},
keywords = {Visual object tracking,computer vision,correlation filters,ing evaluation,track-},
pages = {1--13},
title = {{An Experimental Survey on Correlation Filter-based Tracking}},
year = {2014}
}
@article{Kurzejamski2015,
abstract = {This paper presents a method for analysis of the vote space created from the local features extraction process in a multi-detection system. The method is opposed to the classic clustering approach and gives a high level of control over the clusters composition for further verification steps. Proposed method comprises of the graphical vote space presentation, the proposition generation, the two-pass iterative vote aggregation and the cascade filters for verification of the propositions. Cascade filters contain all of the minor algorithms needed for effective object detection verification. The new approach does not have the drawbacks of the classic clustering approaches and gives a substantial control over process of detection. Method exhibits an exceptionally high detection rate in conjunction with a low false detection chance in comparison to alternative methods.},
archivePrefix = {arXiv},
arxivId = {1601.00781},
author = {Kurzejamski, Grzegorz and Zawistowski, Jacek and Sarwas, Grzegorz},
doi = {10.5220/0005267002520259},
eprint = {1601.00781},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurzejamski, Zawistowski, Sarwas - 2015 - Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features.pdf:pdf},
isbn = {9789897580901},
issn = {1063-6919},
journal = {arXiv:1601.00781 [cs]},
keywords = {approach and gives a,computer vision,extraction process,from the local features,high level,image analysis,in a multi-detection system,method for analysis of,multiple object detection,object localization,pattern matching,the method is opposed,the vote space created,this paper presents a,to the classic clustering},
pages = {252--259},
title = {{Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features}},
url = {http://arxiv.org/abs/1601.00781$\backslash$nhttp://www.arxiv.org/pdf/1601.00781.pdf},
year = {2015}
}
@article{Bergstra2013,
abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters of-ten must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on per-sonal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the al-gorithm and the details of its tuning, it is sometimes di to know whether a given technique is genuinely better, or simply bet-ter tuned. In this work, we propose a meta-modeling ap-proach to support automated hyperparam-eter optimization, with the goal of provid-ing practical tools that replace hand-tuning with a reproducible and unbiased optimiza-Proceedings of the 30 th International Conference on Ma-chine Learning, Atlanta, Georgia, USA, 2013. JMLR: W{\&}CP volume 28. Copyright 2013 by the author(s). tion process. Our approach is to expose the underlying expression graph of how a perfor-mance metric (e.g. classification accuracy on validation examples) is computed from hy-perparameters that govern not only how indi-vidual processing steps are applied, but even which processing steps are included. A hy-perparameter optimization algorithm trans-forms this graph into a program for opti-mizing that performance metric. Our ap-proach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architec-tures.},
author = {Bergstra, J and Yamins, Daniel L K and Cox, D D},
file = {:home/batumon/Desktop/fyp/resources/papers/aut{\_}Making a Science of Model Search$\backslash$: Hyperparameter Optimization
in Hundreds of Dimensions for Vision Architectures.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {115--123},
title = {{Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures}},
url = {http://jmlr.org/proceedings/papers/v28/bergstra13.html},
year = {2013}
}
@article{Nguyen2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Deep Neural Networks are Easily Fooled$\backslash$:
High Confidence Predictions for Unrecognizable Images.pdf:pdf},
isbn = {9781467369640},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
url = {http://arxiv.org/abs/1412.1897},
year = {2014}
}
@article{Jolla2015,
author = {Jolla, La},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}SINGLE UNDERWATER IMAGE ENHANCEMENT USING DEPTH ESTIMATION BASED ON
BLURRINESS.pdf:pdf},
isbn = {9781479983391},
journal = {International Conference on Image Processing (ICIP)},
pages = {2--6},
title = {{SINGLE UNDERWATER IMAGE ENHANCEMENT USING DEPTH ESTIMATION BASED ON BLURRINESS Yan-Tsung Peng , Xiangyun Zhao and Pamela C . Cosman Department of Electrical and Computer Engineering , University of California , San Diego ,}},
year = {2015}
}
@article{MK2014,
author = {MK, Ansar and {Krishnan VR}, Vimal},
doi = {10.5815/ijigsp.2014.12.09},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Performance Evaluation of Image Fusion
Algorithms for Underwater Images-A study
based on PCA and DWT.pdf:pdf},
issn = {20749074},
journal = {International Journal of Image, Graphics and Signal Processing},
number = {12},
pages = {65--69},
title = {{Performance Evaluation of Image Fusion Algorithms for Underwater Images-A study based on PCA and DWT}},
url = {http://www.mecs-press.org/ijigsp/ijigsp-v6-n12/v6n12-9.html},
volume = {6},
year = {2014}
}
@article{Bolme2010,
abstract = {Although not commonly used, correlation filters can track complex objects through rotations, occlusions and other distractions at over 20 times the rate of current state-of-the-art techniques. The oldest and simplest correlation filters use simple templates and generally fail when applied to tracking. More modern approaches such as ASEF and UMACE perform better, but their training needs are poorly suited to tracking. Visual tracking requires robust filters to be trained from a single frame and dynamically adapted as the appearance of the target object changes. This paper presents a new type of correlation filter, a Minimum Output Sum of Squared Error (MOSSE) filter, which produces stable correlation filters when initialized using a single frame. A tracker based upon MOSSE filters is robust to variations in lighting, scale, pose, and nonrigid deformations while operating at 669 frames per second. Occlusion is detected based upon the peak-to-sidelobe ratio, which enables the tracker to pause and resume where it left off when the object reappears.},
archivePrefix = {arXiv},
arxivId = {1404.7584},
author = {Bolme, Dav and Beveridge, J. Ross and Draper, Bruce a. and Lui, Yui Man},
doi = {10.1109/CVPR.2010.5539960},
eprint = {1404.7584},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Visual Object Tracking using Adaptive Correlation Filters.pdf:pdf},
isbn = {978-1-4244-6984-0},
issn = {1063-6919},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2544--2550},
pmid = {23921828},
title = {{Visual object tracking using adaptive correlation filters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539960},
year = {2010}
}
@article{Report2015,
author = {Report, Technical},
doi = {10.13140/RG.2.1.2037.6405},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Underwater Image Contrast Enhancement through Multilevel Histogram Modification Based on Color Channels Percentages.pdf:pdf},
number = {SEPTEMBER 2014},
title = {{Underwater Image Contrast Enhancement through Multilevel Histogram Modification Based on Color Channels Percentages}},
year = {2015}
}
@article{Roberto2014,
author = {Roberto, Paulo},
file = {:home/batumon/Desktop/fyp/resources/related/journal{\_}paper{\_}2016.pdf:pdf},
pages = {1--26},
title = {{T . I . S . N . A . Tratamento Integrado do Sistema Nervoso Aut{\^{o}}nomo}},
year = {2014}
}
@article{Stordal2011,
abstract = {The nonlinear filtering problem occurs in many scientific areas. Sequential Monte Carlo solutions with the correct asymptotic behavior such as particle filters exist, but they are computationally too expensive when working with high-dimensional systems. The ensemble Kalman filter (EnKF) is a more robust method that has shown promising results with a small sample size, but the samples are not guaranteed to come from the true posterior distribution. By approximating the model error with a Gaussian distribution, one may represent the posterior distribution as a sum of Gaussian kernels. The resulting Gaussian mixture filter has the advantage of both a local Kalman type correction and the weighting/resampling step of a particle filter. The Gaussian mixture approximation relies on a bandwidth parameter which often has to be kept quite large in order to avoid a weight collapse in high dimensions. As a result, the Kalman correction is too large to capture highly non-Gaussian posterior distributions. In this paper, we have extended the Gaussian mixture filter (Hoteit et al., Mon Weather Rev 136:317334, 2008) and also made the connection to particle filters more transparent. In particular, we introduce a tuning parameter for the importance weights. In the last part of the paper, we have performed a simulation experiment with the Lorenz40 model where our method has been compared to the EnKF and a full implementation of a particle filter. The results clearly indicate that the new method has advantages compared to the standard EnKF.},
author = {Stordal, Andreas S. and Karlsen, Hans A. and N??vdal, Geir and Skaug, Hans J. and Vall??s, Brice},
doi = {10.1007/s10596-010-9207-1},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Bridging the ensemble Kalman filter and particle
filters$\backslash$: the adaptive Gaussian mixture filter.pdf:pdf},
isbn = {1420-0597},
issn = {14200597},
journal = {Computational Geosciences},
keywords = {Data assimilation,Ensemble Kalman filter,Nonlinear filtering,Particle filters},
number = {2},
pages = {293--305},
pmid = {288216900006},
title = {{Bridging the ensemble Kalman filter and particle filters: The adaptive Gaussian mixture filter}},
volume = {15},
year = {2011}
}
@article{Santana2011,
abstract = {This paper presents an approach that uses planar information (homography matrix) to build a visual 2D occupancy grid map from monocular vision. Initially, a segmentation step is necessary to classify parts of the image in floor or non floor. From this classification is possible to determine which parts of the image are free and which parts of the image are obstacles. Practical results are presented to validate the proposal. ?? 2011 Elsevier B.V.},
author = {Santana, Andr?? M. and Aires, Kelson R T and Veras, Rodrigo M S and Medeiros, Adelardo A D},
doi = {10.1016/j.entcs.2011.11.033},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}An Approach for 2D Visual Occupancy Grid
Map Using Monocular Vision.pdf:pdf},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {Image Segmentation,Monocular Vision,Visual Occupancy Grid Maps},
pages = {175--191},
title = {{An approach for 2D visual occupancy grid map using monocular vision}},
volume = {281},
year = {2011}
}
@article{Borji2015,
abstract = {We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection.},
archivePrefix = {arXiv},
arxivId = {1501.02741},
author = {Borji, Ali and Cheng, Ming-Ming and Jiang, Huaizu and Li, Jia},
doi = {10.1007/978-3-642-33709-3_30},
eprint = {1501.02741},
file = {:home/batumon/Desktop/fyp/resources/papers/Salient Object Detection$\backslash$: A Benchmark.pdf:pdf},
isbn = {9783642337086},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {414--429},
pmid = {26452281},
title = {{Salient Object Detection: A Benchmark}},
url = {http://arxiv.org/abs/1501.02741},
volume = {7573 LNCS},
year = {2015}
}
@article{Rosenblatt1997,
abstract = {An architecture is presented in which distributed task-achieving modules, or behaviours, cooperatively determine a mobile robot's path by voting for each of various possible actions. An arbiter then performs command fusion and selects that action which best satisfies the prioritized goals of the system, as expressed by these votes, without the need to average commands. Command fusion allows multiple goals and constraints to be considered simultaneously. Examples of implemented systems are given, and future research directions in command fusion are discussed. An architecture is presented in which distributed task-achieving modules, or behaviours, cooperatively determine a mobile robot's path by voting for each of various possible actions. An arbiter then performs command fusion and selects that action which best satisfies the prioritized goals of the system, as expressed by these votes, without the need to average commands. Command fusion allows multiple goals and constraints to be considered simultaneously. Examples of implemented systems are given, and future research directions in command fusion are discussed.},
author = {Rosenblatt, Julio K.},
doi = {10.1080/095281397147167},
file = {:home/batumon/Desktop/fyp/resources/papers/plan{\_}DAMN$\backslash$: A Distributed
Architecture
for Mobile Navigation.pdf:pdf},
isbn = {0952813971},
issn = {0952-813X},
journal = {Journal of Experimental {\&} Theoretical Artificial Intelligence},
keywords = {arbitration,behaviors,command fusion,distributed architecture,mobile robots,voting},
number = {2-3},
pages = {339--360},
title = {{DAMN: a distributed architecture for mobile navigation}},
volume = {9},
year = {1997}
}
@article{Todt2004,
abstract = {This work presents a novel technique for embedding color constancy into a saliency-based system for detecting potential landmarks in outdoor environments. Since multiscale color opponencies are among the ingredients determining saliency, the idea is to make such opponencies directly invariant to illumination variations, rather than enforcing the invariance of colors themselves. The new technique is compared against the alternative approach of preprocessing the images with a color constancy procedure before entering the saliency system. The first procedure used in the experimental comparison is the well-known image conversion to chromaticity space, and the second one is based on successive lighting intensity and illuminant color normalizations. The proposed technique offers significant advantages over the preceding two ones since, at a lower computational cost, it exhibits higher stability in front of illumination variations and even of slight viewpoint changes, resulting in a better correspondence of visual saliency to potential landmark elements. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Todt, Eduardo and Torras, Carme},
doi = {10.1016/j.robot.2004.06.003},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Detecting salient cues through illumination-invariant
color ratios.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Color constancy,Visual landmarks,Visual robot navigation,Visual saliency},
number = {2-3},
pages = {111--130},
title = {{Detecting salient cues through illumination-invariant color ratios}},
volume = {48},
year = {2004}
}
@article{Yilmaz2006,
author = {Yilmaz, a and Javed, O and Shah, M},
doi = {10.1145/1177352.1177355},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Object Tracking$\backslash$: A Survey.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
number = {4},
pages = {13--es},
title = {{Object tracking}},
volume = {38},
year = {2006}
}
@article{Wallis2006,
abstract = {We investigate techniques relevant for autonomous computer vision in an underwater environment, in particular techniques enabling students from the University of Bath to create a fully functioning Autonomous Underwater Vehicle for DSTL's first Student Autonomous Underwater Challenge. A variety of segmentation techniques are implemented, in particular taking advantage of domain assumptions of large areas of single colour, and building on the work of previous AUV vision systems. We also implement a selection of techniques for shape representation and classification, notably Fourier Descriptors and Shape Descriptors, and evaluate them in combination with our segmentation techniques with respect to the underwater domain.},
author = {Wallis, Clb},
file = {:home/batumon/Desktop/fyp/resources/related/An Investigation into Computer Vision Techniques for
Underwater Object Recognition.pdf:pdf},
journal = {Dept. of Computer Science, University of Bath},
title = {{An Investigation into Computer Vision Techniques for Underwater Object Recognition}},
url = {http://www.cs.bath.ac.uk/{~}mdv/courses/CM30082/projects.bho/2005-6/wallis-clb-dissertation-2005-6.pdf},
year = {2006}
}
@article{Zhang2015,
author = {Zhang, Jianming and Sclaroff, Stan and Lin, Zhe and Shen, Xiaohui and Price, Brian},
doi = {10.1109/ICCV.2015.165},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Minimum Barrier Salient Object Detection at 80 FPS.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {Iccv},
number = {Iccv},
title = {{Minimum Barrier Salient Object Detection at 80 FPS}},
year = {2015}
}
@article{VanDeWeijer2005,
abstract = {The aim of salient point detection is to find distinctive events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape saliency of the local neighborhood. The majority of these detectors is luminance based which has the disadvantage that the distinctiveness of the local color information is completely ignored. To fully exploit the possibilities of color image salient point detection, color distinctiveness should be taken into account next to shape distinctiveness. In this paper color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Isosalient color derivatives can be closely approximated by ellipsoidal surfaces in color derivative space. Based on this remarkable statistical finding, isosalient derivatives are transformed by color boosting to have equal impact on the saliency. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial improvements in information content are acquired by targeting color salient features. Further, the generality of the method is illustrated by applying color boosting to multiple existing saliency methods.},
author = {{Van De Weijer}, J. and Gevers, Th},
doi = {10.1109/CVPR.2005.93},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Boosting Saliency in Color Image Features.pdf:pdf},
isbn = {0769523722},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {c},
pages = {365--372},
title = {{Boosting saliency in color image features}},
volume = {1},
year = {2005}
}
@article{Kendall,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.0742},
author = {Kendall, Alex and May, C V and College, King},
eprint = {arXiv:1505.0742},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Convolutional networks for real-time 6-DOF camera relocalization.pdf:pdf},
title = {{Convolutional networks for real-time 6-DOF camera relocalization}}
}
@article{Bibuli2007,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Bibuli, Marco and Caccia, Massimo and Lapierre, Lionel},
doi = {10.1002/rob},
eprint = {10.1.1.91.5767},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}True Color Correction of Autonomous Underwater
Vehicle Imagery.PDF:PDF},
isbn = {9783902661623},
issn = {14746670},
journal = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
keywords = {Backstepping,Non-linear control,Path-following},
number = {PART 1},
pages = {81--86},
pmid = {22164016},
title = {{Path-following algorithms and experiments for an autonomous surface vehicle}},
volume = {7},
year = {2007}
}
@article{Danelljan2016,
abstract = {Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 (+5.1{\%} in mean OP), Temple-Color (+4.6{\%} in mean OP), and VOT2015 (20{\%} relative reduction in failure rate). Additionally, our approach is capable of sub-pixel localization, crucial for the task of accu-rate feature point tracking. We also demonstrate the effectiveness of our learning formulation in extensive feature point tracking experiments.},
archivePrefix = {arXiv},
arxivId = {1608.03773},
author = {Danelljan, Martin and Robinson, Andreas and Khan, Fahad Shahbaz and Felsberg, Michael},
doi = {10.1007/978-3-319-46454-1},
eprint = {1608.03773},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Beyond Correlation Filters$\backslash$: Learning Continuous
Convolution Operators for Visual Tracking.pdf:pdf},
isbn = {9783319464541},
issn = {16113349},
journal = {Arxiv},
number = {5},
pages = {1--9},
title = {{Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking}},
year = {2016}
}
@article{Walters2014,
author = {Walters, P and Sauder, N and Nezvadovitz, J and Voight, F and Gray, A and Schwartz, E M and {Walters, P and Sauder, N and Nezvadovitz, J and Voight, F and Gray, A and Schwartz}, EM},
file = {:home/batumon/Desktop/fyp/resources/related/subjugator{\_}2014.pdf:pdf},
journal = {AUVSI Foundation's 17th Annual RoboSub Competition, San Diego, CA},
pages = {1--8},
title = {{SubjuGator 2014: Design and Implementation of a Modular, Fault Tolerant AUV}},
year = {2014}
}
@article{Kwitt,
author = {Kwitt, Roland and Hegenbart, Sebastian and Niethammer, Marc and Hill, U N C Chapel and States, United},
doi = {10.1109/CVPR.2016.16},
file = {:home/batumon/Desktop/fyp/resources/papers/scene{\_}One-Shot Learning of Scene Locations via Feature Trajectory Transfer.pdf:pdf},
title = {{One-Shot Learning of Scene Locations via Feature Trajectory Transfer}}
}
@article{Huang2016,
abstract = {Capabilities of inference and prediction are significant components of visual systems. In this paper, we address an important and challenging task of them: visual path prediction. Its goal is to infer the future path for a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and motion patterns underlying video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of the considered models. Motivated by these observations, we propose a deep learning framework which simultaneously performs deep feature learning for visual representation in conjunction with spatio-temporal context modeling. After that, we propose a unified path planning scheme to make accurate future path prediction based on the analytic results of the context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scene and motion pattern, consequently improving the performance of the visual path prediction task. In order to comprehensively evaluate the model's performance on the visual path prediction task, we construct two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the existing approaches and owns a better generalization capability.},
archivePrefix = {arXiv},
arxivId = {1601.07265},
author = {Huang, Siyu and Li, Xi and Zhang, Zhongfei and He, Zhouzhou and Wu, Fei and Liu, Wei and Tang, Jinhui and Zhuang, Yueting},
eprint = {1601.07265},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2016 - Deep Learning Driven Visual Path Prediction from a Single Image.pdf:pdf},
pages = {1--13},
title = {{Deep Learning Driven Visual Path Prediction from a Single Image}},
url = {http://arxiv.org/abs/1601.07265},
year = {2016}
}
@article{Bojarski2016,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
eprint = {1604.07316},
file = {:home/batumon/Desktop/fyp/resources/papers/car{\_}End to End Learning for Self-Driving Cars.pdf:pdf},
journal = {arXiv:1604},
pages = {1--9},
title = {{End to End Learning for Self-Driving Cars}},
url = {http://arxiv.org/abs/1604.07316},
year = {2016}
}
@article{Cheng,
author = {Cheng, Dongliang and Price, Brian and Cohen, Scott and Brown, Michael S},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Beyond White$\backslash$: Ground Truth Colors for Color Constancy Correction.pdf:pdf},
pages = {298--306},
title = {{Beyond White : Ground Truth Colors for Color Constancy Correction}}
}
@article{Cepeda-Negrete2012,
author = {Cepeda-Negrete, Jonathan and Sanchez-Yanez, Raul E.},
doi = {10.1109/CERMA.2012.12},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Combining Color Constancy and Gamma Correction for Image Enhancement.pdf:pdf},
isbn = {9780769548784},
issn = {07300301},
journal = {Proceedings - 2012 9th Electronics, Robotics and Automotive Mechanics Conference, CERMA 2012},
keywords = {Color constancy,Gamma correction,Gray world,Power spectrum,Retinex},
number = {November},
pages = {25--30},
title = {{Combining color constancy and gamma correction for image enhancement}},
year = {2012}
}
@article{Sedlazeck2011,
abstract = {When adapting computer vision algorithms to underwater imaging, two major differences in image formation occur. While still traveling through the water, light rays are scattered and absorbed depending on their wavelength, creating the typical blue hue and low contrast in underwater images. When entering the underwater housing of the camera, light rays are refracted twice upon passing from water into glass and into air. We propose a simulator for both effects based on physical models for deep sea underwater images captured by cameras in underwater housings with glass port thicknesses in the order of centimeters. Hence, modeling refraction by explicitly computing the correct path of the rays allows to accurately simulate distortions induced by underwater housings. The Jaffe- McGlamery model for effects on color is often used in computer vision algorithms as a base for simplification. We extend this model to incorporate color images, shadows, and several light sources.},
author = {Sedlazeck, Anne and Koch, Reinhard},
doi = {10.2312/PE/VMV/VMV11/049-056},
file = {:home/batumon/Desktop/fyp/resources/papers/simul{\_}Simulating Deep Sea Underwater Images Using Physical
Models for Light Attenuation, Scattering, and Refraction.pdf:pdf},
isbn = {9783905673852},
journal = {Proceedings of the Vision, Modeling, and Visualization Workshop},
pages = {49--56},
title = {{Simulating Deep Sea Underwater Images Using Physical Models for Light Attenuation, Scattering, and Refraction}},
year = {2011}
}
@article{Wu2013,
abstract = {Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.},
author = {Wu, Yi and Lim, Jongwoo and Yang, Ming Hsuan},
doi = {10.1109/CVPR.2013.312},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Online Object Tracking$\backslash$: A Benchmark.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
pages = {2411--2418},
title = {{Online object tracking: A benchmark}},
year = {2013}
}
@article{Moll2015,
abstract = {Motion planning is a key problem in robotics that is concerned with finding a path that satisfies a goal specification subject to constraints. In its simplest form, the solution to this problem consists of finding a path connecting two states, and the only constraint is to avoid collisions. Even for this version of the motion planning problem, there is no efficient solution for the general case [1]. The addition of differential constraints on robot motion or more general goal specifications makes motion planning even harder. Given its complexity, most planning algorithms forego completeness and optimality for slightly weaker notions such as resolution completeness, probabilistic completeness [2], and asymptotic optimality.},
archivePrefix = {arXiv},
arxivId = {1412.6673},
author = {Moll, Mark and Sucan, Ioan A. and Kavraki, Lydia E.},
doi = {10.1109/MRA.2015.2448276},
eprint = {1412.6673},
file = {:home/batumon/Desktop/fyp/resources/papers/plan{\_}Benchmarking Motion Planning Algorithms.pdf:pdf},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {96--102},
title = {{Benchmarking Motion Planning Algorithms: An Extensible Infrastructure for Analysis and Visualization}},
volume = {22},
year = {2015}
}
@article{Cehovin2015,
abstract = {The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments. This makes the cross-paper tracker comparison difficult. Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased towards particular tracking aspects. In this paper we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing towards homogenization of the tracker evaluation methodology. These two measures can be intuitively interpreted and visualized and have been employed by the recent Visual Object Tracking (VOT) challenges as the foundation for the evaluation methodology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05803v2},
author = {{\v{C}}ehovin, Luka and Leonardis, Ale{\v{s}} and Kristan, Matej},
doi = {10.1103/PhysRevD.91.103507},
eprint = {arXiv:1502.05803v2},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Visual object tracking performance measures
revisited.pdf:pdf},
issn = {1057-7149},
journal = {arXiv preprint arXiv:1502.05803},
pages = {1--14},
title = {{Visual object tracking performance measures revisited}},
url = {http://arxiv.org/abs/1502.05803$\backslash$nhttp://arxiv.org/pdf/1502.05803},
year = {2015}
}
@article{Redmon2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02640v5},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {arXiv:1506.02640v5},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}You Only Look Once$\backslash$:
Unified, Real-Time Object Detection.pdf:pdf},
journal = {arXiv preprint arXiv:1506.02640},
title = {{You only look once: Unified, real-time object detection}},
year = {2015}
}
@article{Guth2013,
abstract = {Simulators are essential tools, mainly used before practical application projects, in various areas of engineering. The high development cost and complexity are just some of the factors that justify the development of simulators for a given project, which allows among other benefits, conduct tests of the agent behavior against an environment. In robotics, simulators have been used for a long time and consist an important stage in prototyping phases. When the focus is on high depths underwater robotics, simulators have even greater importance since the great difficulty of building robots for this kind of environment. This paper presents a simulator built with Blender IDE integrated with Robotic Operating System (ROS) for underwater robotics applications, using sensory data from a simulated physics game engine. We present results of applications using this project as well as discussion of the key benefits of its development. It is observed an important potential for projects in underwater robotics, and its appeal in demonstrations and examples for learning robotics and 3D modeling. {\textcopyright} 2013 MTS.},
author = {Guth, F and Botelho, S and Amaral, M},
file = {:home/batumon/Desktop/fyp/resources/papers/simul{\_}UnderwaterSimulatorforRoboticsApplicationsinHighDepths.pdf:pdf},
isbn = {9780933957404},
journal = {OCEANS 2013 MTS/IEEE San Diego Conference: An Ocean in Common},
keywords = {3-d modeling,3D modeling,Agent behavior,Blending,Building robots,Development costs,High depths,Oceanography,Robotics,Robotics applications,Sensory data,Simulator,Simulators,Three dimensional,Underwater Robotics,Underwater robotics},
number = {January},
title = {{Underwater simulator for robotics applications in high depths}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84896373348{\&}partnerID=40{\&}md5=8466e05e19429b9a1f538053581b3896},
year = {2013}
}
@article{Sevilla-Lara2016,
abstract = {Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, we model the motion on roads with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine motion plus deviations. We then pose the flow estimation problem using a novel formulation of localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our semantic flow method achieves the lowest error of any published monocular method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos.},
archivePrefix = {arXiv},
arxivId = {1603.03911},
author = {Sevilla-Lara, Laura and Sun, Deqing and Jampani, Varun and Black, Michael J},
doi = {10.1109/CVPR.2016.422},
eprint = {1603.03911},
file = {:home/batumon/Desktop/fyp/resources/papers/flow{\_}Optical Flow with Semantic Segmentation and Localized Layers.pdf:pdf},
journal = {Cvpr},
pages = {1--10},
title = {{Optical Flow with Semantic Segmentation and Localized Layers}},
year = {2016}
}
@article{Posner2016,
abstract = {This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data – as commonly encountered in robotics applications – and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise},
archivePrefix = {arXiv},
arxivId = {1602.00991},
author = {Posner, Ingmar and Ondruska, Peter},
eprint = {1602.00991},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Deep Tracking$\backslash$: Seeing Beyond Seeing Using Recurrent Neural Networks.pdf:pdf},
journal = {The Thirtieth AAAI Conference on Artificial Intelligence (AAAI)},
title = {{Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks}},
url = {http://www.robots.ox.ac.uk/{~}mobile/Papers/2016AAAI{\_}ondruska.pdf$\backslash$nhttps://www.youtube.com/watch?v=cdeWCpfUGWc},
year = {2016}
}
@article{M2016,
author = {M, Alex Raj S and Abhilash, S and Supriya, M H},
doi = {10.9790/4200-0602013033},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}A Comparative Study of Various Methods for Underwater Image
Enhancement and Restoration.pdf:pdf},
keywords = {color correction,enhancement,haze,transmission map,underwater image},
number = {2},
pages = {30--33},
title = {{A Comparative Study of Various Methods for Underwater Image Enhancement and Restoration}},
volume = {6},
year = {2016}
}
@article{Engel2014,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by ltering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the e ect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
author = {Engel, Jakob and Sch, Thomas and Cremers, Daniel},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}LSD-SLAM$\backslash$: Large-Scale
Direct Monocular SLAM.pdf:pdf},
pages = {834--849},
title = {{Direct Monocular SLAM}},
year = {2014}
}
@article{Thomas,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00110v1},
author = {Thomas, Christopher},
eprint = {arXiv:1606.00110v1},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}sal{\_}OpenSalicon$\backslash$: An Open Source Implementation of the Salicon Saliency Model.pdf:pdf},
title = {{OpenSalicon : An Open Source Implementation of the Salicon Saliency Model}}
}
@article{Winschel2016,
abstract = {Current top performing object recognition systems build on object proposals as a preprocessing step. Object proposal algorithms are designed to generate candidate regions for generic objects, yet current approaches are limited in capturing the vast variety of object characteristics. In this paper we analyze the error modes of the state-of-the-art Selective Search object proposal algorithm and suggest extensions to broaden its feature diversity in order to mitigate its error modes. We devise an edge grouping algorithm for handling objects without clear boundaries. To further enhance diversity, we incorporate the Edge Boxes proposal algorithm, which is based on fundamentally different principles than Selective Search. The combination of segmentations and edges provides rich image information and feature diversity which is essential for obtaining high quality object proposals for generic objects. For a preset amount of object proposals we achieve considerably better results by using our combination of different strategies than using any single strategy alone.},
archivePrefix = {arXiv},
arxivId = {1603.04308},
author = {Winschel, Anton and Lienhart, Rainer and Eggert, Christian},
eprint = {1603.04308},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Winschel, Lienhart, Eggert - 2016 - Diversity in Object Proposals.pdf:pdf},
number = {C},
title = {{Diversity in Object Proposals}},
url = {http://arxiv.org/abs/1603.04308},
year = {2016}
}
@article{Nam2015,
abstract = {We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.},
archivePrefix = {arXiv},
arxivId = {1510.07945},
author = {Nam, Hyeonseob and Han, Bohyung},
doi = {10.1109/CVPR.2016.465},
eprint = {1510.07945},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Learning Multi-Domain Convolutional Neural Networks for Visual Tracking.pdf:pdf},
journal = {arXiv preprint arXiv:1510.07945},
pages = {4293--4302},
title = {{Learning Multi-Domain Convolutional Neural Networks for Visual Tracking}},
url = {http://arxiv.org/abs/1510.07945},
year = {2015}
}
@article{Claessens2015,
author = {Claessens, Rik and de Waal, Alta and de Villiers, Pieter and Penders, Ate and Pavlin, Gregor and Tuyls, Karl},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}Multi-Agent Target Tracking using
Particle Filters enhanced with Context Data.pdf:pdf},
isbn = {978-1-4503-3413-6},
issn = {15582914},
journal = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
keywords = {bayesian networks,distributed information fusion,particle filters,situation {\&}{\#}38; threat assessment,target tracking},
number = {Cd},
pages = {1933--1934},
title = {{Multi-Agent Target Tracking Using Particle Filters Enhanced with Context Data: (Demonstration)}},
url = {http://dl.acm.org/citation.cfm?id=2772879.2773511},
year = {2015}
}
@article{Ondruska2015,
abstract = {We present the first pipeline for real-time volumetric surface reconstruction and dense 6DoF camera tracking running purely on standard, off-the-shelf mobile phones. Using only the embedded RGB camera, our system allows users to scan objects of varying shape, size, and appearance in seconds, with real-time feedback during the capture process. Unlike existing state of the art methods, which produce only point-based 3D models on the phone, or require cloud-based processing, our hybrid GPU/CPU pipeline is unique in that it creates a connected 3D surface model directly on the device at 25Hz. In each frame, we perform dense 6DoF tracking, which continuously registers the RGB input to the incrementally built 3D model, minimizing a noise aware photoconsistency error metric. This is followed by efficient key-frame selection, and dense per-frame stereo matching. These depth maps are fused volumetrically using a method akin to KinectFusion, producing compelling surface models. For each frame, the implicit surface is extracted for live user feedback and pose estimation. We demonstrate scans of a variety of objects, and compare to a Kinect-based baseline, showing on average 1:5cm error. We qualitatively compare to a state of the art point-based mobile phone method, demonstrating an order of magnitude faster scanning times, and fully connected surface models.},
author = {Ondruska, Peter and Kohli, Pushmeet and Izadi, Shahram},
doi = {10.1109/TVCG.2015.2459902},
file = {:home/batumon/Desktop/fyp/resources/papers/mobile{\_}MobileFusion$\backslash$: Real-time Volumetric Surface Reconstruction and
Dense Tracking On Mobile Phones.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D object scanning,mobile computing,surface reconstruction},
number = {11},
pages = {1251--1258},
title = {{MobileFusion: Real-Time Volumetric Surface Reconstruction and Dense Tracking on Mobile Phones}},
volume = {21},
year = {2015}
}
@article{Rasmussen2007,
abstract = {We introduce a framework for object detection and tracking in video of natural outdoor scenes based on fast per-frame segmentations using Felzenszwalb's graph-based algorithm. Region boundaries obtained at multiple scales are first temporally filtered to detect stable structures to be considered as object hypotheses. Depending on object type, these are then classified using a priori appearance characteristics such as color and texture and geometric attributes derived from the Hough transform. We describe preliminary results on image sequences taken from low-flying aircraft in which object categories are relevant to UAVs, consisting of sky, ground, and navigationally-useful ground features such as roads and pipelines.},
author = {Rasmussen, Christopher},
doi = {10.1007/978-3-540-76858-6_5},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Superpixel Analysis for
Object Detection and Tracking
with Application to UAV Imagery.pdf:pdf},
isbn = {978-3-540-76857-9},
issn = {03029743},
journal = {Advances in Visual Computing},
pages = {46--55},
title = {{Superpixel analysis for object detection and tracking with application to UAV imagery}},
url = {http://www.springerlink.com/index/J28026VJ8603U387.pdf},
year = {2007}
}
@article{Zhao2015,
abstract = {Low-level saliency cues or priors do not produce good enough saliency detection results especially when the salient object presents in a low-contrast background with confusing visual appearance. This issue raises a serious problem for conventional approaches. In this paper, we tackle this problem by proposing a multi-context deep learning framework for salient object detection. We employ deep Convolutional Neural Networks to model saliency of objects in images. Global context and local context are both taken into account, and are jointly modeled in a unified multi-context deep learning framework. To provide a better initialization for training the deep neural networks, we investigate different pre-training strategies, and a task-specific pre-training scheme is designed to make the multi-context modeling suited for saliency detection. Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated. Our approach is extensively evaluated on five public datasets, and experimental results show significant and consistent improvements over the state-of-the-art methods.},
author = {Zhao, Rui and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
doi = {10.1109/CVPR.2015.7298731},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Saliency Detection by Multi-Context Deep Learning.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1265--1274},
title = {{Saliency detection by multi-context deep learning}},
volume = {07-12-June},
year = {2015}
}
@article{Stewart2015,
abstract = {Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes.},
archivePrefix = {arXiv},
arxivId = {1506.04878},
author = {Stewart, Russell and Andriluka, Mykhaylo},
doi = {10.1109/CVPR.2016.255},
eprint = {1506.04878},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}End-to-end people detection in crowded scenes.pdf:pdf},
journal = {Arxiv},
pages = {9},
title = {{End-to-end people detection in crowded scenes}},
url = {http://arxiv.org/abs/1506.04878},
year = {2015}
}
@article{Bani??2014,
author = {Bani??, Nikola and Lon??ari??, Sven},
doi = {10.1109/ICDSP.2014.6900684},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Color Rabbit$\backslash$: Guiding the Distance of Local
Maximums in Illumination Estimation.pdf:pdf},
isbn = {9781479946129},
journal = {International Conference on Digital Signal Processing, DSP},
keywords = {Color constancy,Illumination estimation,Image enhancement,Retinex,White balance},
number = {Cd},
pages = {345--350},
title = {{Color rabbit: Guiding the distance of local maximums in illumination estimation}},
volume = {2014-Janua},
year = {2014}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Danelljan2016a,
abstract = {Visual object tracking is a challenging computer vision problem with numerous real-world applications. This pa-per investigates the impact of convolutional features for the visual tracking problem. We propose to use activations from the convolutional layer of a CNN in discriminative correlation filter based tracking frameworks. These acti-vations have several advantages compared to the standard deep features (fully connected layers). Firstly, they miti-gate the need of task specific fine-tuning. Secondly, they contain structural information crucial for the tracking prob-lem. Lastly, these activations have low dimensionality. We perform comprehensive experiments on three benchmark datasets: OTB, ALOV300++ and the recently introduced VOT2015. Surprisingly, different to image classification, our results suggest that activations from the first layer pro-vide superior tracking performance compared to the deeper layers. Our results further show that the convolutional fea-tures provide improved results compared to standard hand-crafted features. Finally, results comparable to state-of-the-art trackers are obtained on all three benchmark datasets.},
author = {Danelljan, Martin and Hager, Gustav and Khan, Fahad Shahbaz and Felsberg, Michael},
doi = {10.1109/ICCVW.2015.84},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Convolutional Features for Correlation Filter Based Visual Tracking.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {Benchmark testing,Convolution,Correlation,Feature extraction,Standards,Target tracking,Visualization},
pages = {621--629},
title = {{Convolutional Features for Correlation Filter Based Visual Tracking}},
volume = {2016-Febru},
year = {2016}
}
@article{Srinivas2016,
abstract = {Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative – that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e., deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision – convolutional neural networks (CNNs). We start with “AlexNet” as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision.},
archivePrefix = {arXiv},
arxivId = {1601.06615},
author = {Srinivas, Suraj and Sarvadevabhatla, Ravi Kiran and Mopuri, Konda Reddy and Prabhu, Nikita and Kruthiventi, Srinivas S. S. and Babu, R. Venkatesh},
doi = {10.3389/frobt.2015.00036},
eprint = {1601.06615},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivas et al. - 2016 - A Taxonomy of Deep Convolutional Neural Nets for Computer Vision.pdf:pdf},
issn = {2296-9144},
journal = {Frontiers in Robotics and AI},
keywords = {convolutional neural networks,deep learning,obje,object classification,recurrent neural networks},
number = {January},
pages = {1--13},
title = {{A Taxonomy of Deep Convolutional Neural Nets for Computer Vision}},
url = {http://journal.frontiersin.org/article/10.3389/frobt.2015.00036},
volume = {2},
year = {2016}
}
@article{Jarrett2016,
abstract = {(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6$\backslash${\%} recognition rate on Caltech-101. With the addition of convolutional training, a 77$\backslash${\%} recognition was obtained on the CIfAR-10 dataset.},
archivePrefix = {arXiv},
arxivId = {1606.01535},
author = {Jarrett, Kevin and Kvukcuoglu, Koray and Gregor, Karol and LeCun, Yann},
eprint = {1606.01535},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}What is the Best Feature Learning Procedure in
Hierarchical Recognition Architectures?.pdf:pdf},
pages = {1--19},
title = {{What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?}},
url = {http://arxiv.org/abs/1606.01535},
year = {2016}
}
@article{Burkardt2014,
abstract = {—CUAUV Gemini is the new 2013-2014 autonomous underwater vehicle (AUV) designed and built by a team of 40 undergraduate students at Cornell University. Completed in a ten month design cycle, the vehicle was fully modeled using CAD software, extensively simulated with ANSYS, and manufactured almost entirely in-house. With 15 years of aggregate research and development to enhance AUV technology by CUAUV students, Gemini is the culmination of previous designs. Gemini presents a stronger, lighter, and more agile platform with increased capabilities over previous vehicles. New ad-vancements include full vehicle control of six degrees of freedom, a dual-hull cantilevered electronics rack, overhauled wire routing for electrical systems, and significant software changes which yield improved mission reliability and robustness. Gemini's sensor suite includes machine vision cameras, compasses, inertial measurement units (IMUs), a Teledyne RDI Explorer doppler velocity log, a depth sensor, an internal pressure sensor, and a hydrophone array. Returning features include a vacuum-assisted sealing system, hot-swappable battery pods, pneumatic actu-ators, unified serial communications, and a flexible mission software architecture.},
author = {Burkardt, Markus and Beaulieu, Kevin and Chang, Corey and Chayanupatkul, Sean and Chiang, Eric and Du, Brook and Esslinger, Kent and Fu, Zheng and Goes, Christopher and Gong, Edward and Halpern, Andrew and Hamada, Melissa and Heidel, Jeff and Henderson, Kylar and Jackson, Brendon and Jatusiripitak, Napon and {Hyun Kim}, Dong and Lee, Moonyoung and Lei, Kevin and Levy, Noah and Luo, Anne and Mahoney, Mike and Malcoci, Alexandru and Marathe, Mihir and Mehr, Leo and Porter, Zach and Risvold, Katie and Sanderson, Charles and Sew, Daryl and Sim, Kuen-Kuen and Spitzer, Alex and Thiel, Ellen and Ting, Alex and Tome, Samuel and Turkmen, Baturay and Wang, Nancy and Wijaya, Alvin and Wu, James and Wu, Shuqing and Xing, Cheng and Zhang, Rene and Zhang, Mei},
file = {:home/batumon/Desktop/fyp/resources/related/CornellPaper2014.pdf:pdf},
title = {{Cornell University Autonomous Underwater Vehicle: Design and Implementation of the Gemini AUV}},
year = {2014}
}
@article{isard1998condensation,
  title={Condensation—conditional density propagation for visual tracking},
  author={Isard, Michael and Blake, Andrew},
  journal={International journal of computer vision},
  volume={29},
  number={1},
  pages={5--28},
  year={1998},
  publisher={Springer}
}
@article{suzuki1985topological,
  title={Topological structural analysis of digitized binary images by border following},
  author={Suzuki, Satoshi and others},
  journal={Computer Vision, Graphics, and Image Processing},
  volume={30},
  number={1},
  pages={32--46},
  year={1985},
  publisher={Elsevier}
}
@article{Goroshin2015,
author = {Goroshin, Rostislav},
file = {:home/batumon/Desktop/fyp/resources/papers/unsup{\_}Unsupervised Feature Learning in Computer Vision.pdf:pdf},
journal = {Thesis},
number = {September},
pages = {100},
title = {{Unsupervised Feature Learning in Computer Vision}},
year = {2015}
}
@article{Papadopoulos2016,
abstract = {Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning algorithm. Our scheme iterates between re-training the detector, re-localizing objects in the training images, and human verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Extensive experiments on PASCAL VOC 2007 show that (1) using human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detectors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme substantially reduces total annotation time by a factor 6x-9x.},
archivePrefix = {arXiv},
arxivId = {1602.08405},
author = {Papadopoulos, Dim P. and Uijlings, Jasper R. R. and Keller, Frank and Ferrari, Vittorio},
doi = {10.1109/CVPR.2016.99},
eprint = {1602.08405},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papadopoulos et al. - 2016 - We don't need no bounding-boxes Training object class detectors using only human verification.pdf:pdf},
number = {1},
title = {{We don't need no bounding-boxes: Training object class detectors using only human verification}},
url = {http://arxiv.org/abs/1602.08405},
year = {2016}
}
@article{Kuen2014,
abstract = {Visual representation is crucial for visual tracking method's performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target's motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to perform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favorably against several state-of-the-art trackers.},
archivePrefix = {arXiv},
arxivId = {1604.04144},
author = {Kuen, Jason and Lim, Kian Ming and Lee, Chin Poo},
doi = {10.1016/j.patcog.2015.02.012},
eprint = {1604.04144},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Self-taught learning of a deep invariant representation for visual tracking via
temporal slowness principle.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Deep learning,Invariant representation,Self-taught learning,Temporal slowness,Visual tracking},
number = {10},
pages = {2964--2982},
title = {{Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle}},
url = {http://dx.doi.org/10.1016/j.patcog.2015.02.012},
volume = {48},
year = {2014}
}
@article{Gidaris2015,
abstract = {We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework. For implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent state-of-the-art object detection systems, helping them to boost their performance. Furthermore, it sets a new state-of-the-art on PASCAL VOC2012 test set achieving mAP of 74.8{\%}. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of bounding box proposal methods.},
archivePrefix = {arXiv},
arxivId = {1511.07763},
author = {Gidaris, Spyros and Komodakis, Nikos},
doi = {10.1109/CVPR.2016.92},
eprint = {1511.07763},
file = {:home/batumon/Desktop/fyp/resources/papers/det-LocNet$\backslash$: Improving Localization Accuracy for Object Detection.pdf:pdf},
pages = {789--798},
title = {{LocNet: Improving Localization Accuracy for Object Detection}},
url = {http://arxiv.org/abs/1511.07763},
volume = {5},
year = {2015}
}
@article{Liu2016,
abstract = {This paper proposes a novel semi-supervised method on object recognition. First, based on Boost Picking, a universal algorithm, Boost Picking Teaching (BPT), is proposed to train an effective binary-classifier just using a few labeled data and amounts of unlabeled data. Then, an ensemble strategy is detailed to synthesize multiple BPT-trained binary-classifiers to be a high-performance multi-classifier. The rationality of the strategy is also analyzed in theory. Finally, the proposed method is tested on two databases, CIFAR-10 and CIFAR-100. Using 2{\%} labeled data and 98{\%} unlabeled data, the accuracies of the proposed method on the two data sets are 78.39{\%} and 50.77{\%} respectively.},
archivePrefix = {arXiv},
arxivId = {1603.07957},
author = {Liu, Fuqiang and Bi, Fukun and Chen, Liang},
eprint = {1603.07957},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Bi, Chen - 2016 - Object Recognition Based on Amounts of Unlabeled Data.pdf:pdf},
number = {equation 2},
title = {{Object Recognition Based on Amounts of Unlabeled Data}},
url = {http://arxiv.org/abs/1603.07957},
volume = {1},
year = {2016}
}
@article{Ng,
author = {Ng, Thomas and Team, Wenjie and Wei, Goh Eng and Hui, Ng and Lynnette, Xian and John, Alex and Yongchang, Huang and Thien, Nguyen Duc and Huan, Hoang The and Jin, Tan Soon and Yeow, Tey Kee},
file = {:home/batumon/Desktop/fyp/resources/related/Design and Implementation of Bumblebee AUV 3.0.pdf:pdf},
pages = {1--9},
title = {{Design and Implementation of BumbleBee AUV}}
}
@article{Kim2014,
abstract = {In this paper, we introduce a novel approach to automatically detect salient regions in an image. Our approach consists of global and local features, which complement each other to compute a saliency map. The first key idea of our work is to create a saliency map of an image by using a linear combination of colors in a high-dimensional color space. This is based on an observation that salient regions often have distinctive colors compared with backgrounds in human perception, however, human perception is complicated and highly nonlinear. By mapping the low-dimensional red, green, and blue color to a feature vector in a high-dimensional color space, we show that we can composite an accurate saliency map by finding the optimal linear combination of color coefficients in the high-dimensional color space. To further improve the performance of our saliency estimation, our second key idea is to utilize relative location and color contrast between superpixels as features and to resolve the saliency estimation from a trimap via a learning-based algorithm. The additional local features and learning-based algorithm complement the global estimation from the high-dimensional color transform-based algorithm. The experimental results on three benchmark datasets show that our approach is effective in comparison with the previous state-of-the-art saliency estimation methods.},
author = {Kim, Jiwhan and Han, Dongyoon and Tai, Yu Wing and Kim, Junmo},
doi = {10.1109/CVPR.2014.118},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Salient{\_}Region{\_}Detection{\_}via{\_}High-Dimensional{\_}Color{\_}Transform.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {883--890},
title = {{Salient region detection via high-dimensional color transform}},
year = {2014}
}
@article{Chen2003,
abstract = {In this self-contained survey/review paper, we system- atically investigate the roots of Bayesian filtering as well as its rich leaves in the literature. Stochastic filtering theory is briefly reviewed with emphasis on nonlinear and non-Gaussian filtering. Following the Bayesian statistics, different Bayesian filtering techniques are de- veloped given different scenarios. Under linear quadratic Gaussian circumstance, the celebrated Kalman filter can be derived within the Bayesian framework. Optimal/suboptimal nonlinear filtering tech- niques are extensively investigated. In particular, we focus our at- tention on the Bayesian filtering approach based on sequential Monte Carlo sampling, the so-called particle filters. Many variants of the particle filter as well as their features (strengths and weaknesses) are discussed. Related theoretical and practical issues are addressed in detail. In addition, some other (new) directions on Bayesian filtering are also explored.},
archivePrefix = {arXiv},
arxivId = {10.1.1.107.7415},
author = {Chen, Z H E},
doi = {10.1.1.107.7415},
eprint = {10.1.1.107.7415},
file = {:home/batumon/Desktop/fyp/resources/papers/bayes{\_}Bayesian Filtering$\backslash$: From Kalman Filters to
Particle Filters, and Beyond.pdf:pdf},
issn = {00220949},
journal = {Statistics},
keywords = {adaptive systems lab,mcmaster university},
number = {1},
pages = {1--69},
pmid = {18288504},
title = {{Bayesian Filtering: From Kalman Filters to Particle Filters, and Beyond}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.7415{\&}rep=rep1{\&}type=pdf},
volume = {182},
year = {2003}
}
@article{Gulcehre2013,
abstract = {We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hy- pothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experi- ments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first level of the two-tiered MLP is pre-trained with intermediate level targets being the presence of sprites at each location, while the second level takes the output of the first level as input and predicts the final task target binary event. The two-tiered MLP architecture, with a few tens of thou- sand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, de- cision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not per- formed is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective lo- cal minima.},
archivePrefix = {arXiv},
arxivId = {1301.4083},
author = {G{\"{u}}l{\c{c}}ehre, {\c{C}}ağlar and Bengio, Yoshua},
eprint = {1301.4083},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Knowledge Matters$\backslash$: Importance of Prior Information for
Optimization.pdf:pdf},
issn = {1532-4435},
journal = {arXiv preprint arXiv:1301.4083},
keywords = {curriculum,deep learning,evolution of culture,neural networks,optimization},
pages = {1--12},
title = {{Knowledge Matters : Importance of Prior Information for Optimization}},
url = {http://arxiv.org/abs/1301.4083},
volume = {17},
year = {2013}
}
@article{Collins2003,
author = {Collins, Robert T and Liu, Yanxi},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}On-Line Selection of
Discriminative Tracking Features.pdf:pdf},
isbn = {0769519504},
number = {Iccv},
pages = {2--8},
title = {{On-Line Selection of Discriminative Tracking Features {\pounds}}},
year = {2003}
}
@article{Yang2014,
abstract = {在几年内,由于VJ的开拓性工作,人脸检测领域一直是研究热点.虽然很多人使用了一些强大的学习方法来提升效果,但是特征表示任然没有满足对自然场景人脸各种表象差异的描述需求.为了解决这个问题,我们将通道特征的概念借用到人脸检测领域,它能将图像通道扩充到不同的类型,例如梯度幅值和方向梯度直方图,因而在一个简单的形式下包含更多的信息.我们使用了一个通用变种,称为聚合通道特征(ACF),仔细探索了特征设计的方方面面,并发现了一个具有更好性能的多尺度特征.为了解决自然场景下不同的人脸朝向,我们提出了一个具有重排序和检测调整的多尺度检测框架.使用VJ的学习框架,我们的基于ACF的多尺度方法在AFW和FDDB上达到了现有最好方法的效果,且在VGA图像上达到42FPS.},
archivePrefix = {arXiv},
arxivId = {1407.4023},
author = {Yang, Bin and Yan, Junjie and Lei, Zhen and Li, Stan Z.},
doi = {10.1109/BTAS.2014.6996284},
eprint = {1407.4023},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Aggregate Channel Features for Multi-view Face Detection.pdf:pdf},
isbn = {9781479935840},
journal = {IJCB 2014 - 2014 IEEE/IAPR International Joint Conference on Biometrics},
title = {{Aggregate channel features for multi-view face detection}},
year = {2014}
}
@article{Bianco2015,
abstract = {Recovering correct or at least realistic colors of underwater scenes is a very challenging issue for imaging techniques, since illumination conditions in a refractive and turbid medium as the sea are seriously altered. The need to correct colors of underwater images or videos is an important task required in all image-based applications like 3D imaging, navigation, documentation, etc. Many imaging enhancement methods have been proposed in literature for these purposes. The advantage of these methods is that they do not require the knowledge of the medium physical parameters while some image adjustments can be performed manually (as histogram stretching) or automatically by algorithms based on some criteria as suggested from computational color constancy methods. One of the most popular criterion is based on gray-world hypothesis, which assumes that the average of the captured image should be gray. An interesting application of this assumption is performed in the Ruderman opponent color space l$\alpha$$\beta$, used in a previous work for hue correction of images captured under colored light sources, which allows to separate the luminance component of the scene from its chromatic components. In this work, we present the first proposal for color correction of underwater images by using l$\alpha$$\beta$ color space. In particular, the chromatic components are changed moving their distributions around the white point (white balancing) and histogram cutoff and stretching of the luminance component is performed to improve image contrast. The experimental results demonstrate the effectiveness of this method under gray-world assumption and supposing uniform illumination of the scene. Moreover, due to its low computational cost it is suitable for real-time implementation.},
author = {Bianco, G. and Muzzupappa, M. and Bruno, F. and Garcia, R. and Neumann, L.},
doi = {10.5194/isprsarchives-XL-5-W5-25-2015},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}A NEW COLOR CORRECTION METHOD FOR UNDERWATER IMAGING.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {Color correction,Computational color constancy,Ruderman space,Underwater imaging},
number = {5W5},
pages = {25--32},
title = {{A new color correction method for underwater imaging}},
volume = {40},
year = {2015}
}
@article{Blake2011,
abstract = {This book sets out to demonstrate the power of the Markov random field (MRF) in vision. It treats the MRF both as a tool for modeling image data and, coupled with a set of recently developed algorithms, as a means of making inferences about images. The inferences concern underlying image and scene structure to solve problems such as image reconstruction, image segmentation, 3D vision, and object labeling. This chapter is designed to present some of the main concepts used in MRFs, both as a taster and as a gateway to the more detailed chapters that follow, as well as a stand-alone introduction to MRFs.},
author = {Blake, Andrew and Kohli, Pushmeet},
file = {:home/batumon/Desktop/fyp/resources/papers/ml{\_}Introduction to Markov Random Fields.pdf:pdf},
isbn = {0262015773, 9780262015776},
journal = {Markov Random Fields for Vision and Image Processing},
pages = {1--15},
title = {{Introduction to Markov Random Fields}},
year = {2011}
}
@inproceedings{garcia2002way,
  title={On the way to solve lighting problems in underwater imaging},
  author={Garcia, Rafael and Nicosevici, Tudor and Cuf{\'\i}, Xevi},
  booktitle={OCEANS'02 MTS/IEEE},
  volume={2},
  pages={1018--1024},
  year={2002},
  organization={IEEE}
}
@article{buchsbaum1980spatial,
  title={A spatial processor model for object colour perception},
  author={Buchsbaum, Gershon},
  journal={Journal of the Franklin institute},
  volume={310},
  number={1},
  pages={1--26},
  year={1980},
  publisher={Elsevier}
}
@book{land1977retinex,
  title={The retinex theory of color vision},
  author={Land, Edwin H and others},
  year={1977},
  publisher={Citeseer}
}
@inproceedings{finlayson2004shades,
  title={Shades of gray and colour constancy},
  author={Finlayson, Graham D and Trezzi, Elisabetta},
  booktitle={Color and Imaging Conference},
  volume={2004},
  number={1},
  pages={37--41},
  year={2004},
  organization={Society for Imaging Science and Technology}
}
@inproceedings{achanta2009frequency,
  title={Frequency-tuned salient region detection},
  author={Achanta, Radhakrishna and Hemami, Sheila and Estrada, Francisco and Susstrunk, Sabine},
  booktitle={Computer vision and pattern recognition, 2009. cvpr 2009. ieee conference on},
  pages={1597--1604},
  year={2009},
  organization={IEEE}
}
@article{uijlings2013selective,
  title={Selective search for object recognition},
  author={Uijlings, Jasper RR and van de Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
  journal={International journal of computer vision},
  volume={104},
  number={2},
  pages={154--171},
  year={2013},
  publisher={Springer}
}
@inproceedings{zitnick2014edge,
  title={Edge boxes: Locating object proposals from edges},
  author={Zitnick, C Lawrence and Doll{\'a}r, Piotr},
  booktitle={European Conference on Computer Vision},
  pages={391--405},
  year={2014},
  organization={Springer}
}
@article{canny1986computational,
  title={A computational approach to edge detection},
  author={Canny, John},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={679--698},
  year={1986},
  publisher={IEEE}
}
@article{hu1962visual,
  title={Visual pattern recognition by moment invariants},
  author={Hu, Ming-Kuei},
  journal={IRE transactions on information theory},
  volume={8},
  number={2},
  pages={179--187},
  year={1962},
  publisher={IEEE}
}
@article{Mathe2012,
abstract = {One of the most widely used strategies for visual object detection is based on exhaustive spatial hypothesis search. While methods like sliding windows have been successful and effective for many years, they are still brute-force, in-dependent of the image content and the visual category be-ing searched. In this paper we present principled sequential models that accumulate evidence collected at a small set of image locations in order to detect visual objects effectively. By formulating sequential search as reinforcement learning of the search policy (including the stopping condition), our fully trainable model can explicitly balance for each class, specifically, the conflicting goals of exploration – sampling more image regions for better accuracy –, and exploita-tion – stopping the search efficiently when sufficiently con-fident about the target's location. The methodology is gen-eral and applicable to any detector response function. We report encouraging results in the PASCAL VOC 2012 ob-ject detection test set showing that the proposed methodol-ogy achieves almost two orders of magnitude speed-up over sliding window methods.},
author = {Mathe, Stefan and Pirinen, Aleksis and Sminchisescu, Cristian},
doi = {10.1109/CVPR.2016.316},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Reinforcement{\_}Learning{\_}for{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
pages = {2894--2902},
title = {{Reinforcement Learning for Visual Object Detection}},
year = {2012}
}
@article{Gracias2008,
abstract = {A common problem in video surveys in very shallow waters is the presence of strong light fluctuations, due to sun light refraction. Refracted sunlight casts fast moving patterns, which can significantly degrade the quality of the acquired data. Motivated by the growing need to improve the quality of shallow water imagery, we propose a method to remove sunlight patterns in video sequences. The method exploits the fact that video sequences allow several observations of the same area of the sea floor, over time. It is based on computing the image difference between a given reference frame and the temporal median of a registered set of neighboring images. A key observation is that this difference will have two components with separable spectral content. One is related to the illumination field (lower spatial frequencies) and the other to the registration error (higher frequencies). The illumination field, recovered by lowpass filtering, is used to correct the reference image. In addition to removing the sunflickering patterns, an important advantage of the approach is the ability to preserve the sharpness in corrected image, even in the presence of registration inaccuracies. The effectiveness of the method is illustrated in image sets acquired under strong camera motion containing non-rigid benthic structures. The results testify the good performance and generality of the approach.},
author = {Gracias, Nuno and Negahdaripour, Shahriar and Neumann, Laszlo and Prados, Ricard and Garcia, Rafael},
doi = {10.1109/OCEANS.2008.5152111},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}A motion compensated filtering approach to remove
sunlight flicker in shallow water images.pdf:pdf},
isbn = {9781424426201},
issn = {0197-7385 ER},
journal = {Oceans 2008},
keywords = {image enhancement,lighting artifacts,sunlight flickering,underwater mosaicing},
pages = {1--7},
title = {{A motion compensated filtering approach to remove sunlight flicker in shallow water images}},
year = {2008}
}
@article{Weijer2011,
author = {Weijer, Joost Van De and Gevers, Theo and Bagdanov, Andrew},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}Boosting color saliency in image feature detection.pdf:pdf},
journal = {Pami},
number = {1},
pages = {150--156},
title = {{Boosting color saliency in image feature detection Boosting Color Saliency in Image Feature Detection}},
volume = {28},
year = {2011}
}
@article{Gijsenij2011,
abstract = {Computational color constancy is a fundamental prerequisite for many computer vision applications. This paper presents a survey of many recent developments and state-of-the-art methods. Several criteria are proposed that are used to assess the approaches. A taxonomy of existing algorithms is proposed and methods are separated in three groups: static methods, gamut-based methods, and learning-based methods. Further, the experimental setup is discussed including an overview of publicly available datasets. Finally, various freely available methods, of which some are considered to be state of the art, are evaluated on two datasets.},
author = {Gijsenij, Arjan and Gevers, Theo and {Van De Weijer}, Joost},
doi = {10.1109/TIP.2011.2118224},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Computational Color Constancy$\backslash$: Survey and
Experiments.pdf:pdf},
isbn = {1057-7149 VO  - 20},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Color constancy,Illuminant estimation,Performance evaluation,Survey},
number = {9},
pages = {2475--2489},
pmid = {21342844},
title = {{Computational color constancy: Survey and experiments}},
volume = {20},
year = {2011}
}
@article{Tang2016,
author = {Tang, Ming and Feng, Jiayi},
doi = {10.1109/ICCV.2015.348},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Multi-kernel Correlation Filter for Visual Tracking.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3038--3046},
title = {{Multi-kernel correlation filter for visual tracking}},
volume = {11-18-Dece},
year = {2016}
}
@article{Tsianos,
author = {Tsianos, Konstantinos},
file = {:home/batumon/Desktop/fyp/resources/papers/plan{\_}Introduction to State-of-the-art Motion Planning
Algorithms.pdf:pdf},
title = {{Introduction to State-of-the-art Motion Planning Algorithms}}
}
@article{Mirzaei,
author = {Mirzaei, Hamidreza and Funt, Brian and Fraser, Simon},
file = {:home/batumon/Desktop/fyp/resources/papers/feat{\_}A Robust Hue Descriptor.pdf:pdf},
isbn = {9781632660145},
issn = {21692629},
pages = {75--78},
title = {{A Robust Hue Descriptor}}
}
@article{Feng2016,
abstract = {The question why deep learning algorithms perform so well in practice has puzzled machine learning theoreticians and practitioners alike. However, most of well-established approaches, such as hypothesis capacity, robustness or sparseness, have not provided complete explanations, due to the high complexity of the deep learning algorithms and their inherent randomness. In this work, we introduce a new approach{\~{}}$\backslash$textendash{\~{}}ensemble robustness{\~{}}$\backslash$textendash{\~{}}towards characterizing the generalization performance of generic deep learning algorithms. Ensemble robustness concerns robustness of the $\backslash$emph{\{}population{\}} of the hypotheses that may be output by a learning algorithm. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbation is bounded in average, or equivalently, the performance variance of the algorithm is small. Quantifying the ensemble robustness of various deep learning algorithms may be difficult analytically. However, extensive simulations for seven common deep learning algorithms for different network architectures provide supporting evidence for our claims. In addition, as an example for utilizing ensemble robustness, we propose a novel semi-supervised learning method that outperforms the state-of-the-art. Furthermore, our work explains the good performance of several published deep learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1602.02389},
author = {Feng, Jiashi and Zahavy, Tom and Kang, Bingyi and Xu, Huan and Mannor, Shie},
eprint = {1602.02389},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng et al. - 2016 - Ensemble Robustness of Deep Learning Algorithms.pdf:pdf},
journal = {arXiv},
pages = {1--19},
title = {{Ensemble Robustness of Deep Learning Algorithms}},
url = {http://arxiv.org/abs/1602.02389},
year = {2016}
}
@article{Wang2015,
abstract = {Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations. This demands for descriptive and flexible object representations that are also efficient to evaluate for many locations.We propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e., size and aspect ratio). These regionlets are organized in small groups with stable relative positions to delineate fine-grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposals generated from segmentation cues, limiting the evaluation locations to thousands. Our approach achieves very competitive performance on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detection mean average precision of 41.7{\%} on the PASCAL VOC 2007 dataset, 39.7{\%} on the VOC 2010 for 20 object categories. We further develop support pixel integral images to efficiently augment regionlet features with the responses learned by deep convolutional neural networks. Our regionlet based method achieved 20.9{\%} mean average precision on 200 object categories in the ImageNet Large Scale Visual Object Recognition Challenge (ILSVRC 2013).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Wang, Xiaoyu and Yang, Ming and Zhu, Shenghuo and Lin, Yuanqing},
doi = {10.1109/TPAMI.2015.2389830},
eprint = {1111.6189v1},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Regionlets for Generic Object Detection.pdf:pdf},
isbn = {9781479928392},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boosting,Deep Convolutional Neural Network,Object Detection,Object Proposals,Regionlet,Selective Search},
number = {10},
pages = {2071--2084},
pmid = {26353185},
title = {{Regionlets for Generic Object Detection}},
volume = {37},
year = {2015}
}
@article{Huang2016a,
author = {Huang, Xiaoshui and Fan, Lixin and Zhang, Jian and Wu, Qiang and Yuan, Chun},
doi = {10.1109/CVPRW.2016.89},
file = {:home/batumon/Desktop/fyp/resources/papers/depth{\_}Real time complete dense depth reconstruction for a monocular camera.pdf:pdf},
isbn = {9781509014378},
pages = {32--37},
title = {{Real Time Complete Dense Depth Reconstruction for a Monocular Camera}},
year = {2016}
}
@article{Walther2004,
abstract = { For oceanographic research, remotely operated underwater vehicles (ROVs) routinely record several hours of video material each day. Manual processing of such large amounts of video has become a major bottleneck for scientific research based on this data. We have developed an automated system that detects and tracks objects that are of potential interest for human video annotators. By pre-selecting salient targets for track initiation using a selective attention algorithm, we reduce the complexity of multi-target tracking, in particular of the assignment problem. Detection of low-contrast translucent targets is difficult due to variable lighting conditions and the presence of ubiquitous noise from high-contrast organic debris ("marine snow") particles. We describe the methods we developed to overcome these issues and report our results of processing ROV video data.},
author = {Walther, D. and Edgington, D.R. and Koch, C.},
doi = {10.1109/CVPR.2004.1315079},
file = {:home/batumon/Desktop/fyp/resources/related/track{\_}Detection	and	tracking	of	objects	in	underwater
video.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {1063-6919},
journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
number = {August 2016},
pages = {0--5},
title = {{Detection and tracking of objects in underwater video}},
volume = {1},
year = {2004}
}
@article{Gao2013,
abstract = {The double-opponent color-sensitive cells in the primary visual cortex $\backslash$n(V1) of the human visual system (HVS) have long been recognized as the $\backslash$nphysiological basis of color constancy. We introduce a new color constancy model $\backslash$nby imitating the functional properties of the HVS from the retina to the $\backslash$ndouble-opponent cells in V1. The idea behind the model originates from the $\backslash$nobservation that the color distribution of the responses of double-opponent $\backslash$ncells to the input color-biased images coincides well with the light source $\backslash$ndirection. Then the true illuminant color of a scene is easily estimated by $\backslash$nsearching for the maxima of the separate RGB channels of the responses of $\backslash$ndouble-opponent cells in the RGB space. Our systematical experimental $\backslash$nevaluations on two commonly used image datasets show that the proposed model can $\backslash$nproduce competitive results in comparison to the complex state-of-the-art $\backslash$napproaches, but with a simple implementation and without the need for $\backslash$ntraining.},
author = {Gao, Shaobing and Yang, Kaifu and Li, Chaoyi and Li, Yongjie},
doi = {10.1109/ICCV.2013.119},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}A Color Constancy Model with Double-Opponency Mechanisms.pdf:pdf},
isbn = {978-1-4799-2840-8},
issn = {1550-5499},
journal = {Computer Vision (ICCV), 2013 IEEE International Conference on},
keywords = {image colour analysis;HVS;RGB channels;color const},
pages = {929--936},
title = {{A Color Constancy Model with Double-Opponency Mechanisms}},
year = {2013}
}
@article{Sahu2014,
abstract = {This paper is focusing on the methods to improve underwater images because a lot of trouble in underwater image processing due to the low contrast, limiting transmission properties of light in the water (absorption of natural light), underwater images suffer from limited range, non uniform lighting, color diminished, important blur. Due to these reason the quality of images degraded. So here the adopting method in which apply RGB Color Level Stretching and using a narrative RGB Unsharp Masking (USM) filter for underwater image colour quality enhancement is proposed. Unsharp mask (USM) can increase either sharpness or (local) contrast because these are both forms of increasing differences between values, increasing slope – sharpness referring to very small- scale (high frequency) differences, and contrast referring to larger scale (low frequency) differences. By using this method which have obtained better results of underwater image quality enhancement.},
author = {Sahu, Pooja and Gupta, Neelesh and Sharma, Neetu},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}UNDERWATER IMAGE COLOUR
ENHANCEMENT USING COLOUR STRETCHING
TECHNIQUE{\&} USM FILTERS.pdf:pdf},
keywords = {colour level stretching,filter,light correction method,rgb,rgb unsharp masking,usm},
number = {Iocrsem},
pages = {173--180},
title = {{Underwater Image Colour Enhancement Using Colour Stretching Technique {\&} Usm Filters}},
year = {2014}
}
@article{Barngrover2010,
author = {Barngrover, Christopher M .},
file = {:home/batumon/Desktop/fyp/resources/related/Computer Vision Techniques for Underwater Navigation.pdf:pdf},
title = {{Computer Vision Techniques for Underwater Navigation}},
year = {2010}
}
@article{Zhibin2015,
author = {Zhibin, Hong and Chen, Zhe and Wang, Chaohui and Mei, Xue and Prokhorov, Danil and Zhibin, Hong and Chen, Zhe and Wang, Chaohui and Mei, Xue and Prokhorov, Danil and Tracker, Multi-store},
doi = {10.1109/CVPR.2015.7298675},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}MUlti-Store Tracker (MUSTer)$\backslash$: a Cognitive Psychology Inspired Approach to
Object Tracking.pdf:pdf},
isbn = {9781467369640},
title = {{MUlti-Store Tracker ( MUSTer ): a Cognitive Psychology Inspired Approach to Object Tracking To cite this version : MUlti-Store Tracker ( MUSTer ): a Cognitive Psychology Inspired Approach to}},
year = {2015}
}
@article{Kristan2014,
abstract = {The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 pre-decessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website 1 .},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and {\v{C}}ehovin, Luka and Nebehay, Georg and Voj{\'{i}}$\backslash$vr, Tom{\'{a}}{\v{s}} and Fernandez, Gustavo and Others},
doi = {10.1109/ICCVW.2015.79},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}The Visual Object Tracking VOT2015 challenge results.pdf:pdf},
isbn = {9780769557205},
issn = {16113349},
journal = {Eccvw},
pages = {1--27},
title = {{The visual object tracking {\{}VOT2014{\}} challenge results}},
year = {2014}
}
@article{Venkatrayappa2015,
author = {Venkatrayappa, Darshan and Venkatrayappa, Darshan},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}Adaptive Feature Selection for Object Tracking with
Particle Filter.pdf:pdf},
isbn = {9783319117553},
title = {{Adaptive Feature Selection for Object Tracking with Particle Filter e Sidib ´ e , Fabrice Meriaudeau , Philippe To cite this version : Adaptive Feature Selection for Object Tracking with Particle Filter}},
year = {2015}
}
@article{Babenko2011,
author = {Babenko, Boris and Member, Student and Yang, Ming-hsuan and Member, Senior},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Robust Object Tracking with
Online Multiple Instance Learning.pdf:pdf},
journal = {Pami},
title = {{Robust Object Tracking with Online Multiple Instance Learning Robust Object Tracking with Online Multiple Instance Learning}},
year = {2011}
}
@article{Cao2016,
abstract = {Pedestrian detection based on the combination of Convolutional Neural Network (i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved great success. Generally, HOG+LUV are used to generate the candidate proposals and then CNN classifies these proposals. Despite its success, there is still room for improvement. For example, CNN classifies these proposals by the full-connected layer features while proposal scores and the features in the inner-layers of CNN are ignored. In this paper, we propose a unifying framework called Multilayer Channel Features (MCF) to overcome the drawback. It firstly integrates HOG+LUV with each layer of CNN into a multi-layer image channels. Based on the multi-layer image channels, a multi-stage cascade AdaBoost is then learned. The weak classifiers in each stage of the multi-stage cascade is learned from the image channels of corresponding layer. With more abundant features, MCF achieves the state-of-the-art on Caltech pedestrian dataset (i.e., 10.40{\%} miss rate). Using new and accurate annotations, MCF achieves 7.98{\%} miss rate. As many non-pedestrian detection windows can be quickly rejected by the first few stages, it accelerates detection speed by 1.43 times. By eliminating the highly overlapped detection windows with lower scores after the first stage, it's 4.07 times faster with negligible performance loss.},
archivePrefix = {arXiv},
arxivId = {1603.00124},
author = {Cao, Jiale and Pang, Yanwei and Li, Xuelong},
eprint = {1603.00124},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Pang, Li - 2017 - Learning Multilayer Channel Features for Pedestrian Detection.pdf:pdf},
pages = {1--8},
title = {{Learning Multilayer Channel Features for Pedestrian Detection}},
url = {http://arxiv.org/abs/1603.00124},
year = {2016}
}
@article{Li2013,
abstract = {In this paper, we propose a visual saliency detection algorithm from $\backslash$nthe perspective of reconstruction errors. The image boundaries are first $\backslash$nextracted via super pixels as likely cues for background templates, from which $\backslash$ndense and sparse appearance models are constructed. For each image region, we $\backslash$nfirst compute dense and sparse reconstruction errors. Second, the reconstruction $\backslash$nerrors are propagated based on the contexts obtained from K-means clustering. $\backslash$nThird, pixel-level saliency is computed by an integration of multi-scale $\backslash$nreconstruction errors and refined by an object-biased Gaussian model. We apply $\backslash$nthe Bayes formula to integrate saliency measures based on dense and sparse $\backslash$nreconstruction errors. Experimental results show that the proposed algorithm $\backslash$nperforms favorably against seventeen state-of-the-art methods in terms of $\backslash$nprecision and recall. In addition, the proposed algorithm is demonstrated to be $\backslash$nmore effective in highlighting salient objects uniformly and robust to $\backslash$nbackground noise.},
author = {Li, Xiaohui and Lu, Huchuan and Zhang, Lihe and Ruan, Xiang and Yang, Ming Hsuan},
doi = {10.1109/ICCV.2013.370},
file = {:home/batumon/Desktop/fyp/resources/papers/Saliency Detection via Dense and Sparse Reconstruction.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2976--2983},
title = {{Saliency detection via dense and sparse reconstruction}},
year = {2013}
}
@article{Kloss2009,
author = {Kloss, Guy K.},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Colour{\_}Constancy{\_}using{\_}von{\_}Kries{\_}Transformations-Colour{\_}Constancy{\_}goes{\_}to{\_}the{\_}Lab.pdf:pdf},
journal = {Res. Lett. Inf. Math. Sci.},
keywords = {bradford transformation,cie lab,colour constancy,colour spaces,tion,von kries transforma-,white point estimation},
pages = {19--33},
title = {{Colour Constancy using von Kries Transformations}},
volume = {13},
year = {2009}
}
@article{Matsumura2008,
abstract = {Object tracking is an important role in many areas of engineering such as surveillance and computer vision. Over the last few years, particle filters have been proved to be powerful tools for object tracking. To adopt particle filters, the number of tracking objects is very important. In this paper, we propose a method of entrance detection in the specific areas using two-state particle filters. We demonstrate the algorithm on a set of real object targets whose image contains a number of peoples, bicycles and cars.},
author = {Matsumura, Ryo and Okamura, Kenshiro},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}Object Detection and Tracking Using Particle
Filtering.pdf:pdf},
keywords = {multi-target detection,object tracking,particle filter},
number = {1},
pages = {75--86},
title = {{パーティクルフィルタを用いた進入検出と物体追跡 Object Detection and Tracking Using Particle Filter}},
volume = {22},
year = {2008}
}
@article{Sch,
author = {Sch, Thomas and Engel, Jakob},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}Semi-Dense Visual Odometry for AR on a Smartphone.pdf:pdf},
keywords = {3d reconstruction,ar,direct visual odometry,map-,mobile devices,neon,ping,semi-dense,tracking},
title = {{Semi-Dense Visual Odometry for AR on a Smartphone Feature ­ Based}}
}
@article{Bewley2016,
abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
archivePrefix = {arXiv},
arxivId = {1602.00763},
author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
doi = {10.1109/ICIP.2016.7533003},
eprint = {1602.00763},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bewley et al. - 2016 - Simple Online and Realtime Tracking.pdf:pdf},
isbn = {978-1-4673-9961-6},
pages = {5},
title = {{Simple Online and Realtime Tracking}},
url = {http://arxiv.org/abs/1602.00763},
year = {2016}
}
@article{Weiss2001,
author = {Weiss, Yair and {\'{Y}}, {\"{A}} {\"{U}} {\'{Y}} {\^{E}} {\"{U}}},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Deriving intrinsic images from image sequences.PDF:PDF},
isbn = {0769511430},
number = {C},
pages = {1--8},
title = {{Deriving intrinsic images from image sequences ´ µ ´ µ ´ µ ´ µ ´ µ ´ µ {\textperiodcentered} ´ µ ´ µ ´ µ ´ µ {\~{N}} {\`{O}} ´ µ ´ µ}},
volume = {00},
year = {2001}
}
@article{Li2015,
author = {Li, Meng and Cai, Zemin and Wei, Chuliang and Yuan, Ye},
doi = {10.14257/ijca.2015.8.9.29},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}A Survey of Video Object Tracking.pdf:pdf},
issn = {20054297},
keywords = {bayesian estimation,classifier,feature matching,video object tracking},
number = {9},
pages = {303--312},
title = {{A Survey of Video Object Tracking}},
volume = {8},
year = {2015}
}
@article{Pillai2015,
abstract = {In this work, we develop a monocular SLAM- aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is eval- uated on theUWRGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.},
archivePrefix = {arXiv},
arxivId = {1506.01732},
author = {Pillai, Sudeep and Leonard, John J.},
eprint = {1506.01732},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}Monocular SLAM Supported
Object Recognition.pdf:pdf},
journal = {Rss},
keywords = {monocular,slam,vision},
pages = {34--42},
title = {{Monocular SLAM supported object recognition}},
year = {2015}
}
@article{Luo2014,
abstract = {Multiple Object Tracking is an important computer vision task which has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle it, there still exist many issues unsolved. In order to help readers understand this topic, we contribute a systematic and comprehensive review. In the review, we inspect recent advances in various aspects and propose some interesting directions for future research. To our best knowledge, there has not been any review about this topic in the community. We endeavor to provide a thorough review on the development of this problem in the last decades. The main contributions are fourfold: 1) Key aspects in a multiple object tracking system, including how to formulate MOT generally, how to categorize MOT algorithms, what needs to be considered when developing a MOT system and how to evaluate a MOT system, are discussed from the perspective of understanding a topic. We believe this could not only provide researchers, especially new comers to the topic of MOT, a general understanding of the state of the arts, but also help them to comprehend the essential components of a MOT system and the inter-component connection. 2) Instead of enumerating individual works, we discuss existing work according to the various aspects involved in a MOT system. In each aspect, methods are divided into different groups and each group is discussed in details for the principles, advances and drawbacks. 3) We examine experiments of existing publications and give tables which list results on the popular data sets to provide convenient comparison. We also provide some interesting discoveries by analyzing these tables. 4) We offer some potential directions and respective discussions about MOT, which are still open issues and need more research efforts. This would be helpful to identify interesting problems.},
archivePrefix = {arXiv},
arxivId = {1409.7618},
author = {Luo, Wenhan and Xing, Junliang and Zhang, Xiaoqin and Zhao, Xiaowei and Kim, Tae-Kyun},
eprint = {1409.7618},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Multiple Object Tracking$\backslash$: A Literature Review.pdf:pdf},
journal = {arXiv, 1-39. Retrieved from http://arxiv.org/abs/1409.7618},
keywords = {association,dynamic model,multiple object tracking,object detection,observa-,survey,tion model,tracklet},
number = {212},
title = {{Multiple Object Tracking: A Literature Review}},
url = {http://arxiv.org/abs/1409.7618},
volume = {V},
year = {2014}
}
@article{Zhou2007,
author = {Zhou, Jun},
file = {:home/batumon/Desktop/fyp/resources/related/State Estimation Strategies for
Autonomous Underwater Vehicle
Fish Tracking Applications.pdf:pdf},
title = {{State Estimation Strategies for Autonomous Underwater Vehicle Fish Tracking Applications}},
year = {2007}
}
@article{Marks,
author = {Marks, R.L. and Rock, S.M. and Lee, M.J.},
doi = {10.1109/ICSMC.1993.385034},
file = {:home/batumon/Desktop/fyp/resources/related/track{\_}Automatic Object Tracking for an Unmanned Underwater Vehicle
using Real-Time Image Filtering and Correlation.pdf:pdf},
isbn = {0-7803-0911-1},
journal = {Proceedings of IEEE Systems Man and Cybernetics Conference - SMC},
pages = {337--342},
title = {{Automatic object tracking for an unmanned underwater vehicle using real-time image filtering and correlation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=385034}
}
@article{Ondruska2016,
abstract = {In this work we present a novel end-to-end framework for tracking and classifying a robot's surroundings in complex, dynamic and only partially observable real-world environments. The approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations, along with their identity in both visible and occluded areas. To achieve this we first train the network using unsupervised Deep Tracking, a recently proposed theoretical framework for end-to-end space occupancy prediction. We show that by learning to track on a large amount of unsupervised data, the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it's semantic classification. As a result, we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task. Furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real-world robotics applications. We demonstrate the tracking and classification performance of the method on real-world data collected at a busy road junction. Our evaluation shows that the proposed end-to-end framework compares favourably to a state-of-the-art, model-free tracking solution and that it outperforms a conventional one-shot training scheme for semantic classification.},
archivePrefix = {arXiv},
arxivId = {1604.05091},
author = {Ondruska, Peter and Dequaire, Julie and Wang, Dominic Zeng and Posner, Ingmar},
eprint = {1604.05091},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}End-to-End Tracking and Semantic Segmentation
Using Recurrent Neural Networks.pdf:pdf},
title = {{End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1604.05091},
year = {2016}
}
@article{Xiang2015,
author = {Xiang, Yu and Alahi, Alexandre and Savarese, Silvio},
doi = {10.1109/ICCV.2015.534},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Learning to Track$\backslash$: Online Multi-Object Tracking by Decision Making.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {4705--4713},
title = {{Learning to Track: Online Multi-Object Tracking by Decision Making}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Xiang{\_}Learning{\_}to{\_}Track{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{Douglas,
author = {Douglas, Matthew and Dubik, Bradford and Haq, Sarah and Jiang, Yike and Le, Tuquynh and Lemanski, Natalie and Linkins, Samantha and Mcfarland, Genna and Peitzmeier, Sarah and Salins, Mickey and Theisen, Catherine and Douglas, Authors Matthew and Dubik, Bradford and Haq, Sarah and Jiang, Yike and Le, Tuquynh and Lemanski, Natalie and Linkins, Samantha and Murphy, Kaitlyn and Mcfarland, Genna and Rahman, Aziz and Salins, Mickey and Theisen, Catherine and Dorsey, Katy},
file = {:home/batumon/Desktop/fyp/resources/related/AUTONOMOUS TARGET RECOGNITION
AND LOCALIZATION FOR MANIPULATOR
SAMPLING TASKS.pdf:pdf},
title = {{Title of Document :}}
}
@article{Rao2013,
abstract = {This paper gives the survey of the existing developments of Visual object target tracking using particle filter from the last decade and discusses the advantage and disadvantages of various particle filters. A variety of different approaches and algorithms have been proposed in literature. At present most of the work in Visual Object Target Tracking is focusing on using particle filter. The particle filters has the advantage that they deal with nonlinear models and non-Gaussian innovations, and they focus sequentially on the higher density regions of the state space, mostly parallelizable and easy to implement, so it gives a robust tracking framework, as it models the uncertainty and showing good improvement in the recognition performance compared to the kalman filter and other filters like Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF).Various features and classifiers that are used with particle filter are given in this survey.},
author = {Rao, G.Mallikarjuna and Satyanarayana, Ch.},
doi = {10.5815/ijigsp.2013.06.08},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}Visual Object Target Tracking Using Particle
Filter$\backslash$: A Survey.pdf:pdf},
issn = {20749074, 20749082},
journal = {International Journal of Image, Graphics and Signal Processing},
number = {6},
pages = {57--71},
title = {{Visual Object Target Tracking Using Particle Filter: A Survey}},
url = {http://www.mecs-press.org/ijigsp/ijigsp-v5-n6/v5n6-8.html},
volume = {5},
year = {2013}
}
@article{Auv2014,
author = {Auv, Gemini},
file = {:home/batumon/Desktop/fyp/resources/related/Cornell{\_}Journal{\_}Paper{\_}RS15.pdf:pdf},
title = {{Cornell University Autonomous Underwater Vehicle: Design and Implementation of the Gemini AUV}},
year = {2014}
}
@article{Stein2000,
abstract = {We describe a robust method for computing the ego-motion of the$\backslash$nvehicle relative to the road using input from a single camera mounted$\backslash$nnext to the rear view mirror. Since feature points are unreliable in$\backslash$ncluttered scenes we use direct methods where image values in the two$\backslash$nimages are combined in a global probability function. Combined with the$\backslash$nuse of probability distribution matrices, this enables the formulation$\backslash$nof a robust method that can ignore large number of outliers as one would$\backslash$nencounter in real traffic situations. The method has been tested in real$\backslash$nworld environments and has been shown to be robust to glare, rain and$\backslash$nmoving objects in the scene},
author = {Stein, G.P. and Mano, O. and Shashua, a.},
doi = {10.1109/IVS.2000.898370},
file = {:home/batumon/Desktop/fyp/resources/papers/car{\_}A Robust Method for Computing Vehicle Ego-motion.pdf:pdf},
isbn = {0-7803-6363-9},
journal = {Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511)},
number = {Mi},
pages = {362--368},
title = {{A robust method for computing vehicle ego-motion}},
year = {2000}
}
@article{Aulinas2011,
abstract = {Detecting and selecting proper landmarks is a key issue to solve Simultaneous Localization and Mapping (SLAM). In this work, we present a novel approach to perform this landmark detection. Our approach is based on using three sources of information: 1) three-dimensional topological information from SLAM; 2) context information to characterize regions of interest (RoI); and 3) features extracted from these RoIs. Topological information is taken from the SLAM algorithm, i.e. the three-dimensional approximate position of the landmark with a certain level of uncertainty. Contextual information is obtained by segmenting the image into background and RoIs. Features extracted from points of interest are then computed by using common feature extractors such as SIFT and SURF. This information is used to associate new observations with known landmarks obtained from previous observations. The proposed approach is tested under a real unstructured underwater environment using the SPARUS AUV. Results demonstrate the validity of our approach, improving map consistency.},
author = {Aulinas, Josep and Carreras, Marc and Llado, Xavier and Salvi, Joaquim and Garcia, Rafael and Prados, Ricard and Petillot, Yvan R.},
doi = {10.1109/Oceans-Spain.2011.6003474},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Feature extraction for underwater visual SLAM.pdf:pdf},
isbn = {9781457700866},
journal = {OCEANS 2011 IEEE - Spain},
title = {{Feature extraction for underwater visual SLAM}},
year = {2011}
}
@article{Fofi2010,
author = {Fofi, David and Fabrice, M},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}USING VISUAL SALIENCY FOR OBJECT TRACKING WITH PARTICLE FILTERS.pdf:pdf},
pages = {1776--1780},
title = {{Using Visual Saliency for Object Tracking With Particle Filters}},
year = {2010}
}
@article{Borland2004,
author = {Borland, Rives and Brown, Eli and Buescher, James and Caufield, Chris and Chan, Wei Min and Chen, Shawn and Schulze, Karl and Shih, Alex and Sieh, Phillip and Silverthorn, Bryan and Stanish, Mike and Leader, Stenson Team and Wang, Ian and Welch, Sean},
file = {:home/batumon/Desktop/fyp/resources/papers/simul{\_}Design and Control of
an Autonomous Underwater Vehicle
for the RoboSub Competition.pdf:pdf},
title = {{Design and Implementation of an Autonomous Underwater Vehicle for the 2004 AUVSI Underwater Competition}},
year = {2004}
}
@article{Sunderhauf2015,
abstract = {This paper provides a comprehensive investigation of the utility of ConvNet features for robotic place recogni- tion under changing conditions and viewpoint. We focus on analyzing the performance of features extracted from different layers of a Convolutional Network and examine their robustness against changes in viewpoint and environmental conditions.We gain the insight that these features are well suited to be used in place recognition and that the middle layers are very robust against appearance changes while the higher layers become increasingly invariant to viewpoint changes. Experiments are conducted on a number of different challenging datasets. We furthermore demonstrate how the hierarchical structure of the ConvNet features can be exploited by partitioning the search space based on the semantic information that is contained in these layers.},
archivePrefix = {arXiv},
arxivId = {1501.04158v1},
author = {S{\"{u}}nderhauf, Niko and Shirazi, Sareh and Dayoub, Feras and Upcroft, Ben and Milford, Michael},
doi = {10.1109/IROS.2015.7353986},
eprint = {1501.04158v1},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}On the Performance of ConvNet Features for Place Recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Computer vision,Feature extraction,Real-time systems,Robustness,Semantics,Visualization},
pages = {4297--4304},
title = {{On the performance of ConvNet features for place recognition}},
volume = {2015-Decem},
year = {2015}
}
@article{Maldonado-ram,
author = {Maldonado-ram, Alejandro and Torres-m, L Abril},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Using Super-color-pixels descriptors for Tracking Relevant Cues
in Underwater Environments with Poor Visibility Conditions.pdf:pdf},
title = {{Using Super-color-pixels descriptors for Tracking Relevant Cues in Underwater Environments with Poor Visibility Conditions}}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}VERY D EEP C ONVOLUTIONAL N ETWORKS
FOR L ARGE -S CALE I MAGE R ECOGNITION.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Kempka2016,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Kempka, Micha{\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\'{s}}kowski, Wojciech},
eprint = {1605.02097},
file = {:home/batumon/Desktop/fyp/resources/papers/learn{\_}ViZDoom$\backslash$: A Doom-based AI Research Platform
for Visual Reinforcement Learning.pdf:pdf},
keywords = {deep reinforcement learning,first-person perspective games,fps,neural networks,video games,visual learning,visual-based reinforcement learning},
title = {{ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning}},
url = {http://arxiv.org/abs/1605.02097},
year = {2016}
}
@article{Ng2009,
author = {Ng, KK and Delp, EJ},
doi = {10.1117/12.807311},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}New Models For Real-Time Tracking Using Particle Filtering.pdf:pdf},
journal = {IS{\&}T/SPIE Electronic Imaging},
keywords = {color features,edge features,histograms,motion estimation,object tracking,particle filtering},
pages = {72570B--72570B--12},
title = {{New models for real-time tracking using particle filtering}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=813435},
year = {2009}
}
@article{Cheng2014,
abstract = {Color constancy is a well studied topic in color vision. Methods are generally categorized as: 1) low-level statistical methods; 2) gamut-based methods; 3) and learning-based methods. In this work, we distinguish methods depending on whether they work directly from color values (i.e. color domain) or from values obtained from the image's spatial information (e.g. image gradients/frequencies). We show that spatial information does not provide any additional information that cannot be obtained directly from the color distributions and that the indirect aim of spatial domain methods is to obtain large color differences for estimating the illumination direction. This finding allows us to develop a simple and efficient illumination estimation method that chooses bright and dark pixels using a projection distance in the color distribution and then applies PCA to estimate the illumination direction. Our method gives state-of-the-art results on existing public color constancy data sets as well as on our newly collected data set containing 1736 images from 8 different high-end consumer cameras.},
author = {Cheng, Dongliang and Prasad, Dilip K. and Brown, Michael S.},
doi = {10.1364/JOSAA.31.001049},
file = {:home/batumon/Desktop/fyp/resources/papers/col{\_}Illuminant Estimation for Color Constancy$\backslash$:
Why spatial domain methods work and the role of the color
distribution.pdf:pdf},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A},
number = {5},
pages = {1049},
pmid = {24979637},
title = {{Illuminant estimation for color constancy: why spatial-domain methods work and the role of the color distribution}},
url = {http://www.comp.nus.edu.sg/{~}brown/pdf/ColorConstancyJOSAv10.pdf$\backslash$nhttps://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-31-5-1049},
volume = {31},
year = {2014}
}
@article{Okuma2004,
abstract = {The problem of tracking a varying number of non-rigid objects has two major difficulties. First, the observation models and target distributions can be highly non-linear and non-Gaussian. Second, the presence of a large, varying number of objects creates complex interactions with overlap and ambiguities. To surmount these difficulties, we introduce a vision system that is capable of learning, detecting and tracking the objects of interest. The system is demonstrated in the context of tracking hockey players using video sequences. Our approach combines the strengths of two successful algorithms: mixture particle filters and Adaboost. The mixture particle filter [17] is ideally suited to multi-target tracking as it assigns a mixture component to each player. The crucial design issues in mixture particle filters are the choice of the proposal distribution and the treatment of objects leaving and entering the scene. Here, we construct the proposal distribution using a mixture model that incorporates information from the dynamic models of each player and the detection hypotheses generated by Adaboost. The learned Adaboost proposal distribution allows us to quickly detect players entering the scene, while the filtering process enables us to keep track of the individual players. The result of interleaving Adaboost with mixture particle filters is a simple, yet powerful and fully automatic multiple object tracking system.},
author = {Okuma, Kenji and Taleghani, Ali and Freitas, Nando De and Little, James J and Lowe, David G},
doi = {10.1007/978-3-540-24670-1_3},
file = {:home/batumon/Desktop/fyp/resources/papers/pf{\_}A Boosted Particle Filter$\backslash$:
Multitarget Detection and Tracking.pdf:pdf},
isbn = {9783540219842},
issn = {03029743},
journal = {Proceedings of the 8th European Conference on Computer Vision - ECCV 2004},
pages = {28--39},
title = {{A Boosted Particle Filter : Multitarget Detection and Tracking}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-24670-1{\_}3$\backslash$nhttp://www.cs.ubc.ca/{~}little/links/linked-papers/kenji-eccv2004.pdf},
year = {2004}
}
@article{Rybski2003,
abstract = {This paper addresses the problem of simultaneous localization and mapping (SLAM) for the case of very small, resource-limited robots which have poor odometry and can typically only carry a single monocular camera. We propose a modification to the standard SLAM algorithm in which the assumption that the robots can obtain metric distance/bearing information to landmarks is relaxed. Instead, the robot registers a distinctive sensor "signature", based on its current location, which is used to match robot positions. In our formulation of this non-linear estimation problem, we infer implicit position measurements from an image recognition algorithm. The iterated form of the extended Kalman filter (IEKF) is employed to process all measurements.},
author = {Rybski, Paul E. and Gini, Mari and Papanikolopoulos, Nikolaos},
doi = {10.1109/IROS.2003.1250627},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}Appearance-Based Minimalistic Metric SLAM.pdf:pdf},
isbn = {0780378601},
journal = {Proceedings. 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2003. (IROS 2003).},
number = {October},
pages = {194--199},
title = {{Appearance-Based Minimalistic Metric SLAM}},
year = {2003}
}
@article{Gundogdu2016,
author = {Gundogdu, Erhan and Koc, Aykut and Solmaz, Berkan and Hammoud, Riad I and Alatan, A Aydın},
doi = {10.1109/CVPRW.2016.43},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Evaluation of Feature Channels for Correlation-Filter-Based Visual Object
Tracking in Infrared Spectrum.pdf:pdf},
isbn = {9781509014378},
pages = {24--32},
title = {{Evaluation of Feature Channels for Correlation-Filter-Based Visual Object Tracking in Infrared Spectrum}},
year = {2016}
}
@article{Galoogahi2013,
abstract = {Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/ convolution between a multi-channel image and a multi-channel detector/filter which results in a single channel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/ localization tasks where we: (i) exhibit superior performance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.},
author = {Galoogahi, H K and Sim, T and Lucey, S},
doi = {10.1109/ICCV.2013.381},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Multi-Channel Correlation Filters.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Computer Vision (ICCV), 2013 IEEE International Conference on},
keywords = {computer vision;filtering theory;frequency-domain},
pages = {3072--3079},
title = {{Multi-channel Correlation Filters}},
year = {2013}
}
@article{Chen2016,
abstract = {Object proposals greatly benefit object detection task in recent state-of-the-art works, such as R-CNN [2]. However, the existing object proposals usually have low localization accuracy at high intersection over union threshold. To address it, we apply saliency detection to each bounding box to improve their quality in this paper. We first present a geodesic saliency detection method in contour, which is designed to find closed contours. Then, we apply it to each candidate box with multi-sizes, and refined boxes can be easily produced in the obtained saliency maps which are further used to calculate saliency scores for proposal ranking. Experiments on PASCAL VOC 2007 test dataset demonstrate the proposed refinement approach can greatly improve existing models.},
archivePrefix = {arXiv},
arxivId = {1603.04146},
author = {Chen, Shuhan and Li, Jindong and Hu, Xuelong and Zhou, Ping},
doi = {10.1017/CBO9781107415324.004},
eprint = {1603.04146},
file = {:home/batumon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - Saliency Detection for Improving Object Proposals.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Saliency Detection for Improving Object Proposals}},
url = {http://arxiv.org/abs/1603.04146},
year = {2016}
}
@article{Cvpr2016,
author = {Cvpr, Anonymous and Id, Paper},
doi = {10.1109/CVPR.2016.259},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Semantic Channels for Fast Pedestrian Detection.pdf:pdf},
pages = {2360--2368},
title = {{Semantic Channels for Fast Pedestrian Detection}},
year = {2016}
}
@article{Montemerlo2002,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
doi = {10.1.1.16.2153},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}FastSLAM$\backslash$: A Factored Solution to the Simultaneous
Localization and Mapping Problem.pdf:pdf},
isbn = {0262511290},
journal = {Proc. of 8th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence},
number = {2},
pages = {593--598},
title = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
volume = {68},
year = {2002}
}
@article{Felzenszwalb2009,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin- sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, Pedro F and Girshick, Ross B and McAllester, D and Ramanan, Deva},
doi = {10.1109/TPAMI.2009.167},
file = {:home/batumon/Desktop/fyp/resources/papers/det{\_}Object-Detection-with-Discriminatively-Trained-Part-Based-Models--Felzenszwalb-Girshick-McAllester-Ramanan.pdf:pdf},
isbn = {0162-8828 VO - 32},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {9},
pages = {1627--1645},
pmid = {20634557},
title = {{Object Detection with Discriminatively Trained Part Based Models}},
url = {http://cs.brown.edu/{~}pff/papers/lsvm-pami.pdf$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5255236},
volume = {32},
year = {2009}
}
@article{Cao2010,
abstract = {Numerous efforts have been made to detect salient regions in images. Mostly luminance-based saliency models are found in the literature, which ignore the important contribution of color in finding the local distinct image features. Methods about color saliency detection in the literature can only give indication of color salient points or derivatives. In this paper, we present a fast method for detecting the distinct color regions. This model is able to give inference of salient object in the accurate-to-contour level. A segmentation task is also performed based on the proposed color saliency model to show its strength in object outline definition. Finally an evaluation with both the eyetracking and the annotation is achieved for analyzing the model in an extensive dataset. The presented model outperforms the previous work in both tests.},
author = {Cao, Guanqun and Cheikh, Faouzi Alaya},
doi = {10.1109/EUVIP.2010.5699129},
file = {:home/batumon/Desktop/fyp/resources/papers/sal{\_}SALIENT REGION DETECTION WITH OPPONENT COLOR BOOSTING.pdf:pdf},
isbn = {9781424472871},
journal = {2010 2nd European Workshop on Visual Information Processing, EUVIP2010},
keywords = {Boosting,Color imaging,Feature detection,Opponent color,Visual saliency},
pages = {13--18},
title = {{Salient region detection with opponent color boosting}},
year = {2010}
}
@article{Zhang2016,
abstract = {Computer vision algorithms are known to be extremely sensitive to the environmental conditions in which the data is captured, e.g., lighting conditions and target density. Tuning of parameters or choosing a completely new algorithm is often needed to achieve a certain performance level, especially when there is a limitation of the computation source. In this paper, we focus on this problem and propose a framework to adaptively select the "best" algorithm-parameter combination and the computation platform under performance and cost constraints at design time, and adapt the algorithms at runtime based on real-time inputs. This necessitates developing a mechanism to switch between different algorithms as the nature of the input video changes. Our proposed algorithm calculates a similarity function between a test video scenario and each training scenario, where the similarity calculation is based on learning a manifold of image features that is shared by both the training and test datasets. Similarity between training and test dataset indicates the same algorithm can be applied to both of them and achieve similar performance. We design a cost function with this similarity measure to find the most similar training scenario to the test data. The "best" algorithm under a given platform is obtained by selecting the algorithm with a specific parameter combination that performs the best on the corresponding training data. The proposed framework can be used first offline to choose the platform based on performance and cost constraints, and then online whereby the "best" algorithm is selected for each new incoming video segment for a given platform. In the experiments, we apply our algorithm to the problems of pedestrian detection and tracking. We show how to adaptively select platforms and algorithm-parameter combinations. Our results provide optimal performance on 3 publicly available datasets.},
archivePrefix = {arXiv},
arxivId = {1605.06597},
author = {Zhang, Shu and Zhu, Qi and Roy-Chowdhury, Amit},
eprint = {1605.06597},
file = {:home/batumon/Desktop/fyp/resources/papers/track{\_}Adaptive Algorithm and Platform Selection for
Visual Detection and Tracking.pdf:pdf},
pages = {1--10},
title = {{Adaptive Algorithm and Platform Selection for Visual Detection and Tracking}},
url = {http://arxiv.org/abs/1605.06597},
year = {2016}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/batumon/Desktop/fyp/resources/papers/deep{\_}Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Drews2015,
author = {Drews, Paulo and Nascimento, Erickson R. and Campos, Mario F M and Elfes, Alberto},
doi = {10.1109/IROS.2015.7353501},
file = {:home/batumon/Desktop/fyp/resources/papers/pre{\_}Automatic	Restoration	of	Underwater	Monocular
Sequences	of	Images.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {October},
pages = {1058--1064},
title = {{Automatic restoration of underwater monocular sequences of images}},
volume = {2015-Decem},
year = {2015}
}
@article{Fuentes-Pacheco2012,
abstract = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art. {\textcopyright} 2012 Springer Science+Business Media Dordrecht.},
author = {Fuentes-Pacheco, Jorge and Ruiz-Ascencio, Jos?? and Rend??n-Mancha, Juan Manuel},
doi = {10.1007/s10462-012-9365-8},
file = {:home/batumon/Desktop/fyp/resources/papers/slam{\_}Visual	simultaneous	localization	and
mapping$\backslash$:	a	survey.pdf:pdf},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Data association,Image matching,Salient feature selection,Topological and metric maps,Visual SLAM},
number = {1},
pages = {55--81},
title = {{Visual simultaneous localization and mapping: a survey}},
volume = {43},
year = {2012}
}
@book{gevers2012color,
  title={Color in computer vision: fundamentals and applications},
  author={Gevers, Theo and Gijsenij, Arjan and Van de Weijer, Joost and Geusebroek, Jan-Mark},
  volume={23},
  year={2012},
  publisher={John Wiley \& Sons}
}

@article{wu2015object,
  title={Object tracking benchmark},
  author={Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={9},
  pages={1834--1848},
  year={2015},
  publisher={IEEE}
}

@article{black1998eigentracking,
  title={Eigentracking: Robust matching and tracking of articulated objects using a view-based representation},
  author={Black, Michael J and Jepson, Allan D},
  journal={International Journal of Computer Vision},
  volume={26},
  number={1},
  pages={63--84},
  year={1998},
  publisher={Kluwer Academic Publishers}
}

@inproceedings{zhong2012robust,
  title={Robust object tracking via sparsity-based collaborative model},
  author={Zhong, Wei and Lu, Huchuan and Yang, Ming-Hsuan},
  booktitle={Computer vision and pattern recognition (CVPR), 2012 IEEE Conference on},
  pages={1838--1845},
  year={2012},
  organization={IEEE}
}

@article{mei2011robust,
  title={Robust visual tracking and vehicle classification via sparse representation},
  author={Mei, Xue and Ling, Haibin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={33},
  number={11},
  pages={2259--2272},
  year={2011},
  publisher={IEEE}
}

@inproceedings{he2013visual,
  title={Visual tracking via locality sensitive histograms},
  author={He, Shengfeng and Yang, Qingxiong and Lau, Rynson WH and Wang, Jiang and Yang, Ming-Hsuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2427--2434},
  year={2013}
}

@inproceedings{tuzel2006region,
  title={Region covariance: A fast descriptor for detection and classification},
  author={Tuzel, Oncel and Porikli, Fatih and Meer, Peter},
  booktitle={European conference on computer vision},
  pages={589--600},
  year={2006},
  organization={Springer}
}

@article{hare2016struck,
  title={Struck: Structured output tracking with kernels},
  author={Hare, Sam and Golodetz, Stuart and Saffari, Amir and Vineet, Vibhav and Cheng, Ming-Ming and Hicks, Stephen L and Torr, Philip HS},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={38},
  number={10},
  pages={2096--2109},
  year={2016},
  publisher={IEEE}
}

@inproceedings{sevilla2012distribution,
  title={Distribution fields for tracking},
  author={Sevilla-Lara, Laura and Learned-Miller, Erik},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={1910--1917},
  year={2012},
  organization={IEEE}
}

@inproceedings{grabner2006real,
  title={Real-time tracking via on-line boosting.},
  author={Grabner, Helmut and Grabner, Michael and Bischof, Horst},
  booktitle={BMVC},
  volume={1},
  number={5},
  pages={6},
  year={2006}
}

@article{yang2009context,
  title={Context-aware visual tracking},
  author={Yang, Ming and Wu, Ying and Hua, Gang},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={31},
  number={7},
  pages={1195--1209},
  year={2009},
  publisher={IEEE}
}

@inproceedings{dinh2011context,
  title={Context tracker: Exploring supporters and distracters in unconstrained environments},
  author={Dinh, Thang Ba and Vo, Nam and Medioni, G{\'e}rard},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
  pages={1177--1184},
  year={2011},
  organization={IEEE}
}

@inproceedings{kwon2011tracking,
  title={Tracking by sampling trackers},
  author={Kwon, Junseok and Lee, Kyoung Mu},
  booktitle={2011 International Conference on Computer Vision},
  pages={1195--1202},
  year={2011},
  organization={IEEE}
}

@inproceedings{santner2010prost,
  title={Prost: Parallel robust online simple tracking},
  author={Santner, Jakob and Leistner, Christian and Saffari, Amir and Pock, Thomas and Bischof, Horst},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  pages={723--730},
  year={2010},
  organization={IEEE}
}

@inproceedings{jia2012visual,
  title={Visual tracking via adaptive structural local sparse appearance model},
  author={Jia, Xu and Lu, Huchuan and Yang, Ming-Hsuan},
  booktitle={Computer vision and pattern recognition (CVPR), 2012 IEEE Conference on},
  pages={1822--1829},
  year={2012},
  organization={IEEE}
}

@inproceedings{henriques2012exploiting,
  title={Exploiting the circulant structure of tracking-by-detection with kernels},
  author={Henriques, Jo{\~a}o F and Caseiro, Rui and Martins, Pedro and Batista, Jorge},
  booktitle={European conference on computer vision},
  pages={702--715},
  year={2012},
  organization={Springer}
}

@inproceedings{bao2012real,
  title={Real time robust l1 tracker using accelerated proximal gradient approach},
  author={Bao, Chenglong and Wu, Yi and Ling, Haibin and Ji, Hui},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={1830--1837},
  year={2012},
  organization={IEEE}
}

@inproceedings{kristan2015visual,
  title={The visual object tracking vot2015 challenge results},
  author={Kristan, Matej and Matas, Jiri and Leonardis, Ales and Felsberg, Michael and Cehovin, Luka and Fernandez, Gustavo and Vojir, Tomas and Hager, Gustav and Nebehay, Georg and Pflugfelder, Roman},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={1--23},
  year={2015}
}

@article{nam2015learning,
  title={Learning multi-domain convolutional neural networks for visual tracking},
  author={Nam, Hyeonseob and Han, Bohyung},
  journal={arXiv preprint arXiv:1510.07945},
  year={2015}
}

@inproceedings{danelljan2015learning,
  title={Learning spatially regularized correlation filters for visual tracking},
  author={Danelljan, Martin and Hager, Gustav and Shahbaz Khan, Fahad and Felsberg, Michael},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4310--4318},
  year={2015}
}

@article{zhu2015tracking,
  title={Tracking randomly moving objects on edge box proposals},
  author={Zhu, Gao and Porikli, Fatih and Li, Hongdong},
  journal={arXiv preprint arXiv:1507.08085},
  year={2015}
}

@inproceedings{hua2015online,
  title={Online object tracking with proposal selection},
  author={Hua, Yang and Alahari, Karteek and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3092--3100},
  year={2015}
}

@article{lukevzivc2016deformable,
  title={Deformable Parts Correlation Filters for Robust Visual Tracking},
  author={Luke{\v{z}}i{\v{c}}, Alan and {\v{C}}ehovin, Luka and Kristan, Matej},
  journal={arXiv preprint arXiv:1605.03720},
  year={2016}
}

@article{milan2016mot16,
  title={MOT16: A Benchmark for Multi-Object Tracking},
  author={Milan, Anton and Leal-Taixe, Laura and Reid, Ian and Roth, Stefan and Schindler, Konrad},
  journal={arXiv preprint arXiv:1603.00831},
  year={2016}
}

@inproceedings{hamid2015joint,
  title={Joint probabilistic data association revisited},
  author={Hamid Rezatofighi, Seyed and Milan, Anton and Zhang, Zhen and Shi, Qinfeng and Dick, Anthony and Reid, Ian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3047--3055},
  year={2015}
}

@inproceedings{pirsiavash2011globally,
  title={Globally-optimal greedy algorithms for tracking a variable number of objects},
  author={Pirsiavash, Hamed and Ramanan, Deva and Fowlkes, Charless C},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
  pages={1201--1208},
  year={2011},
  organization={IEEE}
}

@inproceedings{sanchez2016online,
  title={Online multi-target tracking with strong and weak detections},
  author={Sanchez-Matilla, Ricardo and Poiesi, Fabio and Cavallaro, Andrea},
  booktitle={European Conference on Computer Vision},
  pages={84--99},
  year={2016},
  organization={Springer}
}

@inproceedings{choi2015near,
  title={Near-online multi-target tracking with aggregated local flow descriptor},
  author={Choi, Wongun},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3029--3037},
  year={2015}
}

@inproceedings{kieritz2016online,
  title={Online multi-person tracking using Integral Channel Features},
  author={Kieritz, Hilke and Becker, Stefan and H{\"u}bner, Wolfgang and Arens, Michael},
  booktitle={Advanced Video and Signal Based Surveillance (AVSS), 2016 13th IEEE International Conference on},
  pages={122--130},
  year={2016},
  organization={IEEE}
}

@article{dollar2009integral,
  title={Integral channel features},
  author={Doll{\'a}r, Piotr and Tu, Zhuowen and Perona, Pietro and Belongie, Serge},
  year={2009},
  publisher={BMVC Press}
}

@article{zhang2016adaptive,
  title={Adaptive Algorithm and Platform Selection for Visual Detection and Tracking},
  author={Zhang, Shu and Zhu, Qi and Roy-Chowdhury, Amit},
  journal={arXiv preprint arXiv:1605.06597},
  year={2016}
}

@article{collins2005online,
  title={Online selection of discriminative tracking features},
  author={Collins, Robert T and Liu, Yanxi and Leordeanu, Marius},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={10},
  pages={1631--1643},
  year={2005},
  publisher={IEEE}
}

@inproceedings{chau2013automatic,
  title={Automatic parameter adaptation for multi-object tracking},
  author={Chau, Duc Phu and Thonnat, Monique and Bremond, Fran{\c{c}}ois},
  booktitle={International Conference on Computer Vision Systems},
  pages={244--253},
  year={2013},
  organization={Springer}
}

@article{vstajner2015autokit,
  title={Autokit: Automatic machine learning via representation and model search},
  author={{\v{S}}tajner, Tadej},
  year={2015}
}

@inproceedings{feurer2015efficient,
  title={Efficient and robust automated machine learning},
  author={Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2962--2970},
  year={2015}
}

@inproceedings{thornton2013auto,
  title={Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
  author={Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={847--855},
  year={2013},
  organization={ACM}
}

@inproceedings{caruana2004ensemble,
  title={Ensemble selection from libraries of models},
  author={Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={18},
  year={2004},
  organization={ACM}
}

@incollection{NIPS2015_5872,
   title = {Efficient and Robust Automated Machine Learning},
   author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and
             Springenberg, Jost and Blum, Manuel and Hutter, Frank},
   booktitle = {Advances in Neural Information Processing Systems 28},
   editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
   pages = {2962--2970},
   year = {2015},
   publisher = {Curran Associates, Inc.},
   url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf}
}

@inproceedings{komer2014hyperopt,
  title={Hyperopt-sklearn: automatic hyperparameter configuration for scikit-learn},
  author={Komer, Brent and Bergstra, James and Eliasmith, Chris},
  booktitle={ICML workshop on AutoML},
  year={2014}
}

@inproceedings{guyon2016brief,
  title={A brief Review of the ChaLearn AutoML Challenge: Any-time Any-dataset Learning without Human Intervention},
  author={Guyon, Isabelle and Chaabane, Imad and Escalante, Hugo Jair and Escalera, Sergio and Jajetic, Damir and Lloyd, James Robert and Macia, Nuria and Ray, Bisakha and Romaszko, Lukasz and Sebag, Michele and others},
  booktitle={International Conference in Machine Learning (ICML 2016) Workshops},
  year={2016}
}

